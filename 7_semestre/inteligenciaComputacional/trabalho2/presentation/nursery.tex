\documentclass{beamer}

% Setup appearance:

\usetheme{Darmstadt}
\usefonttheme[onlylarge]{structurebold}
\setbeamerfont*{frametitle}{size=\normalsize,series=\bfseries}
\setbeamertemplate{navigation symbols}{}


% Standard packages

\usepackage[brazil]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
%\usepackage[table]{xcolor}
\usepackage{multicol}
\usepackage{textcomp} 

% Setup TikZ
\usepackage{tikz}
\usetikzlibrary{arrows}
\tikzstyle{block}=[draw opacity=0.7,line width=1.4cm]

%diretÃ³rio das figuras
\graphicspath{../article}

\title[Nursery]{%
Nursery%
}

\author[Souza,Medeiros,Santos,Araújo]{
     Danilo~Souza\and
     Hugo~Santos\and
     Iago~Medeiros\and
     Welton~Araújo
     }


\institute[Belém]{
  \inst{1}%
  Universidade Federal do Pará
  }
\date[Belém 2012]{
  16 de Julho de 2013
  }



\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \tableofcontents
\end{frame}

\section{Introdução}
\subsection{Descrição da base de dados}
	\begin{frame}{O Problema}
		\begin{itemize}
			\item Modelo de decisão hierárquica
			\item Desenvolvido para classificar requerimentos à escolas infantis
			\item Muito utilizado nos anos 80 na Slovênia
			\item Os requerimentos rejeitados precisavam de uma explicação objetiva
			\item A decisão final dependia de 3 principais fatores
				\begin{itemize}
					\item Ocupação dos pais e berçário da criança
					\item Estruturas familiar e financeira
					\item Condições sociais e de saúde da família
				\end{itemize}
			\item O modelo foi desenvolvido baseado no sistema DEX [M. Bohanec, V. Rajkovic: Expert system 					for decision making. Sistemica 1(1), pp. 145-157, 1990]
		\end{itemize}		
	\end{frame}
	
	\begin{frame}{Atributos e seus estados}	
		\begin{multicols}{2}
			\begin{itemize}
				\item \textit{parents} (Ocupação dos pais)
				\begin{itemize}
					\item usual, pretentious, great\_pret
				\end{itemize}
			\item \textit{has\_nurs} (Berçário da criança)
				\begin{itemize}
					\item proper, less\_proper, improper, critical, very\_crit 
				\end{itemize}
			\item \textit{form} (Estrutura familia)
				\begin{itemize}
					\item complete, completed, incomplete, foster
				\end{itemize}		
			\item \textit{children} (Número de crianças)
				\begin{itemize}
					\item 1, 2, 3, more
			\end{itemize}						 
			\item \textit{housing} (Condições de moradia)
				\begin{itemize}
					\item convenient, less\_conv, critical 
				\end{itemize}
			\item \textit{finance} (Condições financeiras)
				\begin{itemize}
					\item convenient, inconv
				\end{itemize}
			\item \textit{social} (Condições sociais)
				\begin{itemize}
					\item non-prob, slightly\_prob, problematic 
				\end{itemize}
			\item \textit{health} (Condições de saúde)
				\begin{itemize}
					\item recommended, priority, not\_recom
				\end{itemize}
			\end{itemize}
		\end{multicols}	
	\end{frame}

\subsection{A ferramenta WEKA}

	\begin{frame}
		\begin{itemize}
			\item Programa que possui uma coleção de algoritmos de aprendizagem de máquina para usar em tarefas de mineração de dados
			\item Desenvolvido pelo Machine Learning Group da Universidade de Waikato 
			\item O Weka permite que se trabalhe sobre o dataset e traz ferramentas para: pré-processamento, classificação, regressão, clusterização, regras de associação e visualização 
			\item Todos os nossos resultados foram feitos com o apoio do Weka
		\end{itemize}
	\end{frame}

\section{Rede Neural}
\subsection{O Momentum}

	\begin{frame}{O \textit{momentum}}
		\begin{itemize}
			\item Utilizado para acelerar a convergência de rede
			\item Adiciona uma fração proporcional à alteração enterior no cálculo dos pesos sinapticos
			\item Aumenta a estabilidade do processo de aprendizagem			
		\end{itemize}

	\[w_{ij}(n+1) = w_{ij}(n) + \alpha e_{i}(n)x_{j}(n) + \beta[w_{ij}(n) - w_{ij}(n-1)]\]

	\end{frame}
	
\subsection{Simulação}

	\begin{frame}
		\begin{itemize}
			\item Parâmetros fixos
				\begin{itemize}
					\item Função de ativação: sigmóide
					\item Rede de 1 Camada
				\end{itemize}
			\item Parâmetros variados para simulação
				\begin{itemize}
					\item Poncentagem da base de dados para treinamento da rede
					\item Taxa de aprendizado
					\item Coeficiente de inércia (\textit{momentum})
					\item Número de ciclos (iterações) realizados
				\end{itemize}
			\item Parâmetros analisados nas simulações
				\begin{itemize}
					\item Taxa de erro de classificação
					\item Tempo de construção do modelo
				\end{itemize}
		\end{itemize}
	\end{frame}		
	
\subsection{Resultados}

	\begin{frame}{Momento constante}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosMomento02Treino70VarTxAprendXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosMomento02Treino70VarTxAprendXerro}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Momento constante}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosMomento02Treino70VarTxAprendXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosMomento02Treino70VarAprendXtempo}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Sem momento}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosMomento00Treino70VarAprendXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosMomento00Treino70VarAprendXerro}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Sem momento}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosMomento00Treino70VarAprendXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosMomento00Treino70VarAprendXtempo}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Taxa de aprendizado constante}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosTxAprend02Treino70VarMomentoXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosTxAprend02Treino70VarMomentoXerro}
		\end{figure}
	\end{frame}

	\begin{frame}{Resultados - Taxa de aprendizado constante}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosTxAprend02Treino70VarMomentoXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosTxAprend02Treino70VarMomentoXtempo}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Resultados - Taxa de aprendizado constante e momento = 0 \& momento = 0.2}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TxAprend02Treino70Mom02&00VarCiclosXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TxAprend02Treino70Mom02&00VarCiclosXerro}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Resultados - Taxa de aprendizado constante e momento = 0 \& momento = 0.2}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TxAprend02Treino70Mom02&00VarCiclosXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TxAprend02Treino70Mom02&00VarCiclosXtempo}
		\end{figure}
	\end{frame}	
	
\section{Floresta Randômica}

\begin{frame}{Floresta Randômica (Random Forest)}
		\begin{itemize}
			\item Tudo começou em 1996, com o surgimento do meta-algoritmo de Bagging (Bootstrap Aggregating), por Leo Breiman 
			\item Bootstrap ajuda a reduzir variância e overfitting
				\begin{itemize}
					\item Escolhe aleatoriamente amostras, Di, de um conjunto de treino, D
					\item De cada amostra, encontra o classifier.  
					\item O classifier escolhido será o que mais aparecer, ou seja, tiver maior votos.			
					\item Esse conjunto de treino D tem reposição (ou seja, uma amostra pode ser escolhida mais de uma vez)
					\item Tamanho do conjunto de treino: n
					\item Tamanho do conjunto de amostra: n'
					\item Geralmente n' < n
					\item Quando n' for praticamente n, nota-se uma chance de 63\% de aparecer amostras não repetidas. Daqui que surgiu o nome de Bootstrap
				\end{itemize}
		\end{itemize}		
	\end{frame}

\begin{frame}{Floresta Randômica (Random Forest)}
	\begin{itemize}
		\item Em 2001, Breiman lançou o Random Forest
		\item É um método usado para classificação (e regressão)
		\item O método faz a construção de várias árvores de decisão, com a diferença de que não usa pruning (poda)
		\item Algoritmo:
			\begin{itemize}
			 	\item Escolhe-se aleatoriamente a amostra (bootstrap) Di dentre D
			 	\item Constrói a árvore Ti usando amostra Di
			 	\item Em cada árvore Ti, escolhe aleatoriamente M variáveis e encontra a melhor divisão (split)
			\end{itemize}
		\item No fim, pode-se 
			\begin{itemize}
				\item pegar o voto majoritário (classificação)
				\item calcular a média dos resultados (regressão)
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Exemplo de Random Forest}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.1]{./pictures/RandomForestIago.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:Random Forest}
		\end{figure}
	\end{frame}
	
\subsection{Simulação}

	\begin{frame}
		\begin{itemize}
			\item Parâmetros variados para simulação
				\begin{itemize}
					\item Número de Árvores
					\item Número de Níveis (profundidade)
					\item Número de Atributos Utilizados
					\item Poncentagem da base de dados para treinamento da rede
				\end{itemize}
			\item Parâmetros analisados nas simulações
				\begin{itemize}
					\item Taxa de erro de classificação
					\item Tempo de construção do modelo
				\end{itemize}
		\end{itemize}
	\end{frame}		
	
\subsection{Resultados}

	\begin{frame}{Tempo x Profundidade}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/numArvores10Treino70VarNumNiveisXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:numArvores10Treino70VarNumNiveisXtempo}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Tempo x Treino (Split)}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/NumNiveis16NumArvores100VarTreinoXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:NumNiveis16NumArvores100VarTreinoXtempo}
		\end{figure}
	\end{frame}

	\begin{frame}{Tempo x Árvores}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/NumNiveis16Treino70VarNumArvoresXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:NumNiveis16Treino70VarNumArvoresXtempo}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Erro x Profundidade}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/numArvores10Treino70VarNumNiveisXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:numArvores10Treino70VarNumNiveisXerro}
		\end{figure}
	\end{frame}

	\begin{frame}{Erro x Treino (Split)}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/NumNiveis16NumArvores100VarTreinoXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:NumNiveis16NumArvores100VarTreinoXerro}
		\end{figure}
	\end{frame}	
	
	\begin{frame}{Erro x Árvores}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/NumNiveis16Treino70VarNumArvoresXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:NumNiveis16Treino70VarNumArvoresXerro}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Erro x Atributos selecionados}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/NumArv100NumNiveis16Treino70VarNumFeatXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:NumArv100NumNiveis16Treino70VarNumFeatXerro}
		\end{figure}
	\end{frame}

\section{Prisma}

\begin{frame}{Prisma}
\begin{itemize}
	\item Para cada classe \textit{c} de 1 a \textit{n}:
	\begin{itemize}
		\item \textbf{Passo 1} - Calcular a probabilidade de ocorrência da classe c para cada par-atributo-valor
		\item \textbf{Passo 2} - Selecionar o par-atributo com a probabilidade máxima de ocorrência e crie um subconjunto de treinamento tomado como entrada compreendendo todas as instâncias que o par selecionado (para todas as classes)
		\item \textbf{Passo 3} - Repetir os passos 1 e 2 para este subconjunto até o momento em que ele apresente apenas instâncias da classe \textit{c}. A regra induzida é então a conjunção de todos os pares atributo-valor selecionados na criação deste subconjunto homogêneo.
		\item \textbf{Passo 4} - Remover todas as instâncias, que satisfaçam a regra formada, do conjunto de treinamento.
	\end{itemize}	
	\item Repetir a sequência de 1 a 4 até que todas as instâncias da classe \textit{c} tenham sido removidas.
\end{itemize}
\end{frame}

\begin{frame}{Exemplo - parte I}
\begin{tabular}{|p{2.5cm}|p{2.5cm}|p{2.5cm}|p{2.5cm}|}
\hline 
Atributo-valor & Frequência (P) para classe = recommend & Frequência (T) para atributo-valor & Probabilidade (P/T) \\ 
\hline 
Finance = convenient & 4 & 8 & 0.5 \\ 
\hline 
Finance = inconven & 1 & 8 & 0.125 \\ 
\hline 
Housing = convenient & 1 & 8 & 0.125 \\ 
\hline 
Housing = less\_conv & 4 & 12 & 0.33 \\ 
\hline 
Housing = critical & 1 & 12 & 0.83 \\ 
\hline 
Children = 1 & 0 & 12 & 0 \\ 
\hline 
Children = 2 & 3 & 12 & 0.25 \\ 
\hline 
Children = 3 & 0 & 12 & 0 \\ 
\hline 
Children = more & 4 & 12 & 0.33 \\ 
\hline 
\end{tabular} 
%\begin{figure}
%	\includegraphics[scale=0.7]{./pictures/prismaTabela1.png}
%\end{figure}
\end{frame}

\begin{frame}{Exemplo - parte II}
\begin{figure}
	\includegraphics[scale=0.7]{./pictures/prismaTabela2.png}
\end{figure}
\end{frame}

\subsection{Simulação}
\begin{frame}
	\begin{itemize}
		\item Parâmetro variado na simulação
		\begin{itemize}
			\item Porcentagem da base de dados para treinamento
		\end{itemize}
		\item Parâmetros avaliados nas simulações
		\begin{itemize}
			\item Taxa de erro de classificação
			\item Tempo de construção do modelo
		\end{itemize}
	\end{itemize}
\end{frame}

\subsection{Resultados}
\begin{frame}
\begin{figure}
	\includegraphics[scale=0.7]{./pictures/PrismaVarTreinoXerro.png}
\end{figure}
\end{frame}


\begin{frame}
\begin{figure}
	\includegraphics[scale=0.7]{./pictures/PrismaVarTreinoXtempo.png}
\end{figure}
\end{frame}


\begin{frame}
\begin{figure}
	\includegraphics[scale=0.7]{./pictures/PrismaVarTreinoXerro&tempo.png}
\end{figure}
\end{frame}

\section{Bagging usando Prisma e Rede Neural}
\subsection{Simulação}

\begin{frame}{Testes}
\begin{itemize}
		\item Também foram realizados testes com bagging em conjunto com o Prisma e Rede Neural.
		\item No caso da Rede Neural foi utilizado o momento = 0.2, taxa de aprendizado = 0.2 e número de ciclos = 100. 
		\item No exemplo utilizando o Weka, será variado a quantidade de registros utilizados(porcentagem) e o número de iterações.
\end{itemize}	
	
\end{frame}

\begin{frame}{Prisma - Variando o número de iterações}
\begin{figure}
	\includegraphics[scale=0.5]{./pictures/Bagging(prisma)QtdReg30Reg1VarNumItXerro.png}
\end{figure}
\begin{itemize}
\item utilizando prisma normal o erro = 1.2088\%
\end{itemize}
\end{frame}

\begin{frame}{Prisma - Variando a quantidade de registros}
\begin{figure}
	\includegraphics[scale=0.5]{./pictures/Bagging(prisma)NumIt10Reg1VarQtdRegXerro.png}
\end{figure}
\begin{itemize}
\item utilizando prisma normal o erro = 1.2088\%
\end{itemize}
\end{frame}

\begin{frame}{Rede Neural - Variando o número de iterações}
\begin{figure}
	\includegraphics[scale=0.5]{./pictures/Bagging(rede)QtdReg30Reg1VarNumItXerro.png}
\end{figure}
\begin{itemize}
\item utilizando a Rede neural normal o erro = 0.0257%
\end{itemize}
\end{frame}


\begin{frame}{Rede Neural - Variando a quantidade de registros}
\begin{figure}
	\includegraphics[scale=0.5]{./pictures/Bagging(rede)NumIt10Reg1VarQtdRegXerro.png}
\end{figure}
\begin{itemize}
\item utilizando a Rede neural normal o erro = 0.0257%
\end{itemize}
\end{frame}

\subsection{Conclusões}
\begin{frame}{Conclusões}
\begin{itemize}
\item Para uma base de dados pequena a mistura com o bagging não foi satisfatória.
\item Não foi encontrada implementações que realizem a mistura entre os algoritmos como o Random Forest.
\end{itemize}
\end{frame}

\section{Comparaçãoes e Conclusões}

	\begin{frame}{Conclusões gerais}
		\begin{itemize}
			\item A rede neural encontrou o menor erro, porém com um tempo elevado
			\item O algoritimo prisma obteve um erro satisfatório com o menor tempo
			\item A utilização do algoritmo \textit{bagging} não se mostrou vantajosa para uma base de dados pequena
			\item O algoritmo floresta randômica oferece um bom custo X benefício entre erro e tempo de execução
		\end{itemize}
	\end{frame}
	
	\begin{frame}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.5]{./pictures/ComparacaoErroTodosAlgoritmos.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:ComparacaoErroTodosAlgoritmos}
		\end{figure}
	\end{frame}

	\begin{frame}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.5]{./pictures/ComparacaoTempoTodosAlgoritmos.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:ComparacaoTempoTodosAlgoritmos}
		\end{figure}
	\end{frame}
\end{document}