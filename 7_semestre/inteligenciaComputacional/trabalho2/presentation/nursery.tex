\documentclass{beamer}

% Setup appearance:

\usetheme{Darmstadt}
\usefonttheme[onlylarge]{structurebold}
\setbeamerfont*{frametitle}{size=\normalsize,series=\bfseries}
\setbeamertemplate{navigation symbols}{}


% Standard packages

\usepackage[brazil]{babel}
\usepackage[latin1]{inputenc}
\usepackage{times}
\usepackage[T1]{fontenc}
\usepackage{amsmath}% http://ctan.org/pkg/amsmath
%\usepackage[table]{xcolor}
\usepackage{multicol}
\usepackage{textcomp} 

% Setup TikZ
\usepackage{tikz}
\usetikzlibrary{arrows}
\tikzstyle{block}=[draw opacity=0.7,line width=1.4cm]

%diretÃ³rio das figuras
\graphicspath{../article}

\title[Nursery]{%
Nursery%
}

\author[Souza,Medeiros,Santos,Araújo]{
     Danilo~Souza\and
     Hugo~Santos\and
     Iago~Medeiros\and
     Welton~Araújo
     }


\institute[Belém]{
  \inst{1}%
  Universidade Federal do Pará
  }
\date[Belém 2012]{
  16 de Julho de 2013
  }



\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}
  \tableofcontents
\end{frame}

\section{Introdução}
\subsection{Descrição da base de dados}
	\begin{frame}{O Problema}
		\begin{itemize}
			\item Modelo de decisão hierárquica
			\item Desenvolvido para classificar requerimentos à escolas infantis
			\item Muito utilizado nos anos 80 na Slovênia
			\item Os requerimentos rejeitados precisavam de uma explicação objetiva
			\item A decisão final dependia de 3 principais fatores
				\begin{itemize}
					\item Ocupação dos pais e berçário da criança
					\item Estruturas familiar e financeira
					\item Condições sociais e de saúde da família
				\end{itemize}
			\item O modelo foi desenvolvido baseado no sistema DEX [M. Bohanec, V. Rajkovic: Expert system 					for decision making. Sistemica 1(1), pp. 145-157, 1990]
		\end{itemize}		
	\end{frame}
	
	\begin{frame}{Atributos e seus estados}	
		\begin{multicols}{2}
			\begin{itemize}
				\item \textit{parents} (Ocupação dos pais)
				\begin{itemize}
					\item usual, pretentious, great\_pret
				\end{itemize}
			\item \textit{has\_nurs} (Berçário da criança)
				\begin{itemize}
					\item proper, less\_proper, improper, critical, very\_crit 
				\end{itemize}
			\item \textit{form} (Estrutura familia)
				\begin{itemize}
					\item complete, completed, incomplete, foster
				\end{itemize}		
			\item \textit{children} (Número de crianças)
				\begin{itemize}
					\item 1, 2, 3, more
			\end{itemize}						 
			\item \textit{housing} (Condições de moradia)
				\begin{itemize}
					\item convenient, less\_conv, critical 
				\end{itemize}
			\item \textit{finance} (Condições financeiras)
				\begin{itemize}
					\item convenient, inconv
				\end{itemize}
			\item \textit{social} (Condições sociais)
				\begin{itemize}
					\item non-prob, slightly\_prob, problematic 
				\end{itemize}
			\item \textit{health} (Condições de saúde)
				\begin{itemize}
					\item recommended, priority, not\_recom
				\end{itemize}
			\end{itemize}
		\end{multicols}	
	\end{frame}

\subsection{A ferramente WEKA}

	\begin{frame}
		\begin{itemize}
			\item Programa que possui uma coleção de algoritmos de aprendizagem de máquina para usar em tarefas de mineração de dados
			\item Desenvolvido pelo Machine Learning Group da Universidade de Waikato 
			\item O Weka permite que se trabalhe sobre o dataset e traz ferramentas para: pré-processamento, classificação, regressão, clusterização, regras de associação e visualização 
			\item Todos os nossos resultados foram feitos com o apoio do Weka
		\end{itemize}
	\end{frame}

\section{Rede Neural}
\subsection{O Momentum}

	\begin{frame}{O \textit{momentum}}
		\begin{itemize}
			\item Utilizado para acelerar a convergência de rede
			\item Adiciona uma fração proporcional à alteração enterior no cálculo dos pesos sinapticos
			\item Aumenta a estabilidade do processo de aprendizagem			
		\end{itemize}

	\[w_{ij}(n+1) = w_{ij}(n) + \alpha e_{i}(n)x_{j}(n) + \beta[w_{ij}(n) - w_{ij}(n-1)]\]

	\end{frame}
	
\subsection{A simulação}

	\begin{frame}
		\begin{itemize}
			\item Parâmetros fixos
				\begin{itemize}
					\item Função de ativação: sigmóide
					\item Rede de 1 Camada
				\end{itemize}
			\item Parâmetros variados para simulação
				\begin{itemize}
					\item Poncentagem da base de dados para treinamento da rede
					\item Taxa de aprendizado
					\item Coeficiente de inércia (\textit{momentum})
					\item Número de ciclos (iterações) realizados
				\end{itemize}
			\item Parâmetros analisados nas simulações
				\begin{itemize}
					\item Taxa de erro de classificação
					\item Tempo de construção do modelo
				\end{itemize}
		\end{itemize}
	\end{frame}		
	
\subsection{Resultados}

	\begin{frame}{Momento constante}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosMomento02Treino70VarTxAprendXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosMomento02Treino70VarTxAprendXerro}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Sem momento}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosMomento00Treino70VarAprendXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosMomento00Treino70VarAprendXerro}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Momento constante}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosMomento02Treino70VarTxAprendXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosMomento02Treino70VarAprendXtempo}
		\end{figure}
	\end{frame}

	\begin{frame}{Sem momento}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosMomento00Treino70VarAprendXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosMomento00Treino70VarAprendXtempo}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Taxa de aprendizado constante}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosTxAprend02Treino70VarMomentoXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosTxAprend02Treino70VarMomentoXerro}
		\end{figure}
	\end{frame}

	\begin{frame}{Resultados - Taxa de aprendizado constante}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TodosCiclosTxAprend02Treino70VarMomentoXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TodosCiclosTxAprend02Treino70VarMomentoXtempo}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Resultados - Taxa de aprendizado constante e momento = 0 \& momento = 0.2}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TxAprend02Treino70Mom02&00VarCiclosXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TxAprend02Treino70Mom02&00VarCiclosXerro}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Resultados - Taxa de aprendizado constante e momento = 0 \& momento = 0.2}
		\begin{figure}[h]
			%\centering
			\includegraphics[scale=0.6]{./pictures/TxAprend02Treino70Mom02&00VarCiclosXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:TxAprend02Treino70Mom02&00VarCiclosXtempo}
		\end{figure}
	\end{frame}	
	
\section{Floresta Randômica}

\begin{frame}{Floresta Randômica (Random Forest)}
		\begin{itemize}
			\item Tudo começou em 1996, com o surgimento do meta-algoritmo de Bagging (Bootstrap Aggregating), por Leo Breiman 
			\item Bootstrap ajuda a reduzir variância e overfitting
				\begin{itemize}
					\item Escolhe aleatoriamente amostras, Di, de um conjunto de treino, D
					\item Esse conjunto de treino D tem reposição (ou seja, uma amostra pode ser escolhida mais de uma vez)
					\item Tamanho do conjunto de treino: n
					\item Tamanho do conjunto de amostra: n'
					\item Geralmente n' < n
					\item Quando n' for praticamente n, nota-se uma chance de 63\% de aparecer amostras não repetidas. Daqui que surgiu o nome de Bootstrap
				\end{itemize}
		\end{itemize}		
	\end{frame}

\begin{frame}{Floresta Randômica (Random Forest)}
	\begin{itemize}
		\item Em 2001, Breiman lançou o Random Forest
		\item É um método usado para classificação (e regressão)
		\item O método faz a construção de várias árvores de decisão, com a diferença de que não usa pruning (poda)
		\item Algoritmo:
			\begin{itemize}
			 	\item Escolhe-se aleatoriamente a amostra (bootstrap) Di dentre D
			 	\item Constrói a árvore Ti usando amostra Di
			 	\item Em cada árvore Ti, escolhe aleatoriamente M variáveis e encontra a melhor divisão (split)
			\end{itemize}
		\item No fim, pode-se 
			\begin{itemize}
				\item pegar o voto majoritário (classificação)
				\item calcular a média dos resultados (regressão)
			\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}{Exemplo de Random Forest}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.1]{./pictures/RandomForestIago.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:Random Forest}
		\end{figure}
	\end{frame}
	
\subsection{A simulação}

	\begin{frame}
		\begin{itemize}
			\item Parâmetros variados para simulação
				\begin{itemize}
					\item Número de Árvores
					\item Número de Níveis (profundidade)
					\item Número de Atributos Utilizados
					\item Poncentagem da base de dados para treinamento da rede
				\end{itemize}
			\item Parâmetros analisados nas simulações
				\begin{itemize}
					\item Taxa de erro de classificação
					\item Tempo de construção do modelo
				\end{itemize}
		\end{itemize}
	\end{frame}		
	
\subsection{Resultados}

	\begin{frame}{Tempo x Profundidade}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/numArvores10Treino70VarNumNiveisXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:numArvores10Treino70VarNumNiveisXtempo}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Tempo x Treino (Split)}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/NumNiveis16NumArvores100VarTreinoXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:NumNiveis16NumArvores100VarTreinoXtempo}
		\end{figure}
	\end{frame}

	\begin{frame}{Tempo x Árvores}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/NumNiveis16Treino70VarNumArvoresXtempo.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:NumNiveis16Treino70VarNumArvoresXtempo}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Erro x Profundidade}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/numArvores10Treino70VarNumNiveisXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:numArvores10Treino70VarNumNiveisXerro}
		\end{figure}
	\end{frame}

	\begin{frame}{Erro x Treino (Split)}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/NumNiveis16NumArvores100VarTreinoXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:NumNiveis16NumArvores100VarTreinoXerro}
		\end{figure}
	\end{frame}	
	
	\begin{frame}{Erro x Árvores}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/NumNiveis16Treino70VarNumArvoresXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:NumNiveis16Treino70VarNumArvoresXerro}
		\end{figure}
	\end{frame}
	
	\begin{frame}{Erro x Atributos selecionados}
		\begin{figure}[h]
			\centering
			\includegraphics[scale=0.6]{./pictures/NumArv100NumNiveis16Treino70VarNumFeatXerro.png}
			%\caption{$ \alpha = 0.8$, N_t = 10, T_max = 2048}
			\label{fig:NumArv100NumNiveis16Treino70VarNumFeatXerro}
		\end{figure}
	\end{frame}

\section{Prisma}

\begin{frame}{Prisma}
\begin{itemize}
	\item Para cada classe \textit{c} de 1 a \textit{n}:
	\begin{itemize}
		\item \textbf{Passo 1} - Calcular a probabilidade de ocorrência da classe c para cada par-atributo-valor
		\item \textbf{Passo 2} - Selecionar o pa-atributo com a probabilidade máxima de ocorrência e crie um subconjunto de treinamento tomado como entrada compreendendo todas as instâncias que o par selecionado (para todas as classes)
		\item \textbf{Passo 3} - Repetir os passos 1 e 2 para este subconjunto até o momento em que ele apresente apenas instências da classe c. A regra induzida é então a conjunçao de todos os pares atributo-valor selecionados na criaçao deste subconjunto homogêneo.
		\item \textbf{Passo 4} - Remover todas as instâncias, que satisfaçam a regra formada, do conjunto de treinamento.
	\end{itemize}	
	\item Repetir a sequência de 1 a  até que todas as instâncias da classe \textit{c} tenham sido removidas.
\end{itemize}
\end{frame}

\begin{frame}{Exemplo - parte I}
\begin{figure}
	\includegraphics[scale=0.7]{./pictures/prismaTabela1.png}
\end{figure}
\end{frame}

\begin{frame}{Exemplo - parte II}
\begin{figure}
	\includegraphics[scale=0.7]{./pictures/prismaTabela2.png}
\end{figure}
\end{frame}

\subsection{Simulação}


\begin{frame}{Exemplo - parte II}
\begin{figure}
	\includegraphics[scale=0.7]{./pictures/PrismaVarTreinoXerro.png}
\end{figure}
\end{frame}


\begin{frame}{Exemplo - parte II}
\begin{figure}
	\includegraphics[scale=0.7]{./pictures/PrismaVarTreinoXtempo.png}
\end{figure}
\end{frame}


\begin{frame}{Exemplo - parte II}
\begin{figure}
	\includegraphics[scale=0.7]{./pictures/PrismaVarTreinoXerro&tempo.png}
\end{figure}
\end{frame}

\section{Bagging}



\section{Comparaçãoes e Conclusões}


\end{document}