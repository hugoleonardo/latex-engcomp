\chapter{Conceitos Básicos em Séries Temporais}
\label{cap:conceitos}
%Últimas correções efetuadas em 04/12/07

Este Capítulo apresenta alguns conceitos fundamentais da teoria de séries temporais e introduz os modelos \ac{AR}, \ac{MA}, \ac{ARMA} e \ac{ARIMA} \cite{box94}. É dada uma ênfase especial ao modelo AR, que é essencial para o entendimento do novo modelo de tráfego proposto por este trabalho (vide Capítulo \ref{cap:tarfima}). 
Recomenda-se ao leitor que estiver interessado em exposições detalhadas do assunto a leitura das referências \cite{box76}, \cite{tsay05}, \cite{brockdavis91}, \cite{brockwell96}, \cite{morettin04}, \cite{zivot03} e \cite{percival93}.

\section{Notação de Operadores}
Esta tese adota a seguinte notação de operadores:
\begin{description}
	\item[(a)] operador atrasador de $1$ amostra, denotado por $B$:
						 \begin{equation}
						 	\label{def:op-atraso-unit}
						 	B \rvx_t = \rvx_{t-1}. 
						 \end{equation}
	\item[(b)] operador atrasador de $m$ amostras, denotado por $B^m$, $m \in \mathbb{Z}$: 
						 \begin{equation}
						 	\label{def:op-atraso}
						 	B^m\rvx_t = \rvx_{t-m}. 
						 \end{equation}
						 %O operador $B^m$ também é conhecido como sistema atrasador ideal \cite{opp89}:
						 %\begin{equation}
						 %	\label{def:sist-atraso}
						 %	Y_t = \rvx_{t-m}, 
						 %\end{equation} 
						 %em que $Y_t$ denota a saída do sistema. 
						 A resposta impulsiva\footnote{Resposta ao pulso unitário $\delta_t$.} $h_t$ do sistema atrasador de $m$                    amostras é \cite{opp89}:
						 \begin{equation}
						 	\label{def:resp-imp-atraso}
						 	h_t = \delta_{t-m}, 
						 \end{equation}  
						 e a sua função de transferência\footnote{Definida como a transformada $z$ da resposta impulsiva: 												$H(z)=\overset{\infty}{\underset{t=-\infty}\sum} h_t z^{-t}$.} (ou de sistema) é: 
             \begin{equation}
						 	\label{def:Tz-imp-atraso}
						 	H(z) =  z^{-m}.
						 \end{equation}
  \item[(c)] operador diferença\footnote{As potenciações dos operadores $B$ e $\Delta$ são definidas como 
  $B^j(\rvx_t)=\rvx_{t-j}$ e $\Delta^j(\rvx_t) = \Delta(\Delta^{j-1}(\rvx_t)),\,j \geq1,$ com $\Delta^0(\rvx_t)=\rvx_t$           \cite[pág.19]{brockdavis91}. Polinômios em $B$ e $\Delta$ são manipulados da mesma maneira que polinômios de variáveis reais.   Por exemplo, 
  \[ \Delta^2 \rvx_t = \Delta(\Delta \rvx_t) = (1-B)(1-B) \rvx_t = (1-2B+B^2) \rvx_t = \rvx_t - 2\rvx_{t-1} + \rvx_{t-2}.\]} ou   filtro diferenciador, denotado por $\Delta$:
             \begin{equation}
						 	\label{def:op-dif}
						 	\Delta \rvx_t = \rvx_t - \rvx_{t-1} = (1-B)\rvx_t, 
						 \end{equation}
						 %também denominado sistema de diferença passada
						 %\begin{equation}
						 %	\label{def:sist-dif}
						 %	Y_t = \rvx_t - \rvx_{t-1}, 
						 %\end{equation}
						 O filtro diferenciador tem a resposta impulsiva
						 \begin{equation}
						 	\label{def:resp-imp-dif}
						 	h_t = \delta_t - \delta_{t-1}, 
						 \end{equation}
						 e a função de transferência
						 \begin{equation}
						 	\label{def:Tz-sist-dif}
						 	H(z) = (1-z^{-1}).
						 \end{equation} 
	\item[(d)] operador soma ou filtro integrador\footnote{Também conhecido como sistema acumulador.}, denotado por	$S$:
	           \begin{multline}
						 	\label{def:op-soma}
						 	S \rvx_t = \sum_{i=0}^{\infty}\rvx_{t-i} = \rvx_t + \rvx_{t-1}+ \rvx_{t-2}+\ldots = \\
						 	=(1+B+B^2+\ldots)\rvx_t = (1-B)^{-1}\rvx_t = \Delta^{-1} \rvx_t.
						 \end{multline}
						 A função de transferência do filtro integrador corresponde à inversa da função de sistema                                  definida por (\ref{def:Tz-sist-dif}), ou seja 
						 \begin{equation}
						 	\label{def:Tz-sist-soma}
						 	H(z) = (1-z^{-1})^{-1}.
						 \end{equation}				 
\end{description}

\section{Processos Estocásticos}

A análise de séries temporais supõe que o mecanismo gerador dos dados seja um processo estocástico, o qual é definido a seguir.

\begin{defi}[Processo Estocástico]
\label{def:PE}
Seja $T$ um conjunto arbitrário. Um processo estocástico\footnote{Vários autores da área de séries temporais, tais como Tsay\cite{tsay05} e Zivot e Wang \cite{zivot03}, utilizam o termo ``série temporal'' para o que neste trabalho se define como processo estocástico. Nesta tese, o termo ``série temporal'' é sinônimo de ``realização'' de processo aleatório, conforme será explicado mais adiante.} é uma família $\{\rvx_t, t \in T\}$, tal que, para cada $t \in T$, $\rvx_t$ é uma variável aleatória\footnote{Considere um espaço de probabilidade de referência $(\Omega, \Im, \mathbb{P})$, em que $\Omega$ denota o espaço amostral de um experimento aleatório $\mathcal{H}$, $\Im$ é uma álgebra de Borel \cite[pág. 23]{papoulis91} de eventos definidos em $\Omega$ e $\mathbb{P}$, $\mathbb{P}:\,\Im\rightarrow \mathbb{R}$, é uma medida de probabilidade \cite[pág. 11]{stark02}. Os elementos $\zeta$ de $\Omega$ são os resultados aleatórios de $\mathcal{H}$. A função $\rvx(.)$, que a cada resultado $\zeta$ associa um número real $\rvx(\zeta)$, cujo domínio é $\Omega$ e a imagem é $\mathbb{R}$, é uma variável aleatória somente se a imagem inversa sob $\rvx(.)$ de todos os subconjuntos de Borel em $\mathbb{R}$ (esta coleção de subconjuntos é denominada Álgebra de Borel) são eventos. Isto quer dizer que 
o evento $\{\zeta: \rvx(\zeta) \leq x \} \subset \Im$ para qualquer $x \in \mathbb{R}$ \cite[pág. 59]{stark02}.}
\cite{morettin06}. $\quad \blacksquare$
\end{defi}

Quando o conjunto $T$ da definição \ref{def:PE} é o conjunto dos números inteiros $\mathbb{Z}$, então  $\{\rvx_t\}$ é um processo estocástico de tempo discreto (ou seqüência aleatória); $\{\rvx_t\}$ é um processo estocástico de tempo contínuo se $T$ é tomado como o conjunto dos números reais $\mathbb{R}$. 

A rigor, a variável aleatória $\rvx_t$ da definição \ref{def:PE} é uma função de dois argumentos $\rvx(t,\zeta),\, t\in T,\, \zeta \in \Omega$, uma vez que é definida sobre um espaço amostral $\Omega$. Para cada $\zeta \in \Omega$ tem-se  
uma realização, trajetória ou série temporal $x_t$ \cite[pág. 22]{morettin04}. O conjunto de todas as realizações é denominado \emph{ensemble}. Note-se que cada trajetória é uma função ou seqüência não-aleatória, e que, para cada $t$ fixo, $x_t$ é um número. 

O restante desta tese também adota a notação $\rvx_t$ %, $\rvx(t)$ ou $\rvx[t]$ 
para um processo estocástico $\{\rvx_t,\, t \in T\}$, o que é usual na literatura de engenharia elétrica (vide, por exemplo, os livros \cite{gubner06}, \cite{papoulis91} e \cite{stark02}). 

Um processo $\rvx_t$ é completamente especificado por meio de suas \textbf{distribuições finito-dimensionais} (ou funções de distribuição de probabilidade\footnote{A função de distribuição de probabilidade de primeira ordem $F_\rvx(x)$ também é conhecida como função de distribuição acumulada (\ac{CDF}).} de ordem $n$)  
\begin{equation}
	\label{eq:dist-PE}
	F_{\rvx}(x_1,x_2,\ldots,x_n;t_1,t_2, \ldots,t_n) = P\{\rvx(t_1)\leq x_1, \rvx(t_2)\leq x_2, \ldots, \rvx(t_n)\leq x_n\}
\end{equation}
em que $t_1,t_2, \ldots,t_n$ são elementos quaisquer de $T$ e $n\geq1$. 

A \textbf{função densidade de probabilidade (\ac{PDF}) de ordem $n$}  associada à distribuição finito-dimensional (\ref{eq:dist-PE})\footnote{Supondo-se que a mesma seja diferenciável.} é dada por
\begin{equation}
	\label{eq:pdf-n}
	f_{\rvx}(x_1,x_2,\ldots,x_n;t_1,t_2, \ldots,t_n)  
	= \frac{\partial^n F_{\rvx}(x_1,x_2,\ldots,x_n;t_1,t_2, \ldots,t_n)}{\partial x_1 \partial x_2 \ldots \partial x_n}.
\end{equation}

Aplicando-se a fórmula da densidade de probabilidade condicional
\begin{equation}
	\label{eq:pdf-cond}
	f_{\rvx}(x_k | x_{k-1},\ldots,x_1) = \frac{f_{\rvx}(x_1,\ldots,x_{k-1},x_k)}{f_{\rvx}(x_1,\ldots,x_{k-1})},
\end{equation}
em que $f_{\rvx}(x_1,\ldots,x_{k-1},x_k)$ denota $f_{\rvx}(x_1,\ldots,x_{k-1},x_k;t_1,\ldots,t_{k-1},t_k)$, seguidamente sobre $f_{\rvx}(x_1,\ldots,x_{n-1},x_n)$ obtém-se a regra da cadeia da probabilidade \cite[pág. 362]{stark02}
\begin{equation}
	\label{eq:pdf-chain-rule}
	f_{\rvx}(x_1,x_2,\ldots,x_n)  
	= f_{\rvx}(x_1)f_{\rvx}(x_2|x_1)f_{\rvx}(x_3|x_2,x_1)\ldots f_{\rvx}(x_n | x_{n-1},\ldots,x_1).
\end{equation}

Quando $\rvx_t$ é  uma seqüência de variáveis aleatórias \textbf{mutuamente independentes},   (\ref{eq:pdf-chain-rule}) pode ser reescrita como 
\begin{equation}
	\label{eq:pdf-vas-mut-ind}
	f_{\rvx}(x_1,x_2,\ldots,x_n)  
	= f_{\rvx}(x_1)f_{\rvx}(x_2)\ldots f_{\rvx}(x_n).
\end{equation}
  
\begin{defi}[Processo Puramente Estocástico]
\label{def:PPE}
Um processo $\{\rvx_t, t \in \mathbb{Z}\}$ \textbf{puramente estocástico} é uma seqüência de variáveis aleatórias mutuamente independentes \cite{baccala03}. $\quad \blacksquare$
\end{defi}

\begin{defi}[Processo Independente e Identicamente Distribuído]
\label{def:PIID}
Um processo \textbf{\ac{IID}} $\{\rvx_t, t \in \mathbb{Z}\}$, denotado por $\rvx_t \sim \text{IID}$, é um processo puramente estocástico e identicamente distribuído \cite{baccala03}. $\quad \blacksquare$
\end{defi}

Os conceitos de \textbf{estacionariedade em sentido estrito} e em \textbf{sentido amplo} são enunciados a seguir. 

\begin{defi}[Estacionariedade em Sentido Estrito]
\label{def:PE-ese}
Um processo aleatório $\rvx_t$ é estacionário em sentido estrito se \cite[pág. 297]{papoulis91}
\begin{multline}
	\label{eq:ess}
	F_{\rvx}(x_1,x_2,\ldots,x_n;t_1,t_2, \ldots,t_n) = F_{\rvx}(x_1,x_2,\ldots,x_n;t_1+c,t_2+c, \ldots,t_n+c),\\
						\text{para qualquer} \, c.	\quad \blacksquare
\end{multline}
\end{defi}

De acordo com (\ref{eq:ess}), as propriedades estatísticas de um processo estacionário em sentido estrito $\rvx_t$ não mudam com uma translação do mesmo, ou seja, $\rvx_t$ e $\rvx(t+c)$ possuem as mesmas estatísticas para qualquer $c$. Esta condição é bastante forte e difícil de ser verificada empiricamente, porque muitas vezes não se sabe quais são as distribuições finito-dimensionais que caracterizam um determinado processo aleatório na prática. Sendo assim, adota-se uma caracterização parcial do processo por meio da estimação de momentos de baixa ordem como média, autocorrelação e autocovariância e assume-se uma condição mais fraca de estacionariedade, conhecida como estacionariedade em sentido amplo, que será definida mais adiante (vide definição \ref{def:PE-esa}).  

A média $\mu_{\rvx}(t)$ de $\rvx_t$ é o valor esperado da variável aleatória $\rvx_t$: % \cite[pág. 288]{papoulis91}: 
\begin{equation}
	\label{def:media}
	\mu_{\rvx}(t) = E[\rvx_t]=\int_{-\infty}^{\infty}xf_{\rvx}(x;t)dx,
\end{equation}
em que $f_{\rvx}(x;t)$ denota a função densidade de probabilidade de primeira ordem de $\rvx_t$.

A autocorrelação $R_{\rvx}(t_1,t_2)$ de $\rvx_t$ é o valor esperado do produto $\rvx_{t_1}\rvx_{t_2}$ \cite[pág. 288]{papoulis91}: 
\begin{equation}
	\label{def:autocorr}
	R_{\rvx}(t_1,t_2) = E[\rvx_{t_1}\rvx_{t_2}]=\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} x_1 x_2 f_{\rvx}(x_1,x_2;t_1,t_2)dx_1dx_2,
\end{equation}
em que $f_{\rvx}(x_1,x_2;t_1,t_2)$ denota a função densidade de probabilidade de segunda ordem de $\rvx_t$. 

A autocovariância $C_{\rvx}(t_1,t_2)$ de $\rvx_t$ é a covariância das variáveis aleatórias $\rvx_{t_1}$ e $\rvx_{t_2}$ \cite[pág. 289]{papoulis91}: 
\begin{equation}
	\label{def:autocov}
	C_{\rvx}(t_1,t_2) = R_{\rvx}(t_1,t_2)-\mu_{t_1}\mu_{t_2}.
\end{equation}

O coeficiente de correlação $\rho_{\rvx}(t_1,t_2)$ de $\rvx_t$ é a razão \cite[pág. 294]{papoulis91}: 
\begin{equation}
	\label{def:coef-corr}
	\rho_{\rvx}(t_1,t_2) = \frac{C_{\rvx}(t_1,t_2)}{\sqrt{C_{\rvx}(t_1,t_1)C_{\rvx}(t_2,t_2)}}.
\end{equation}

A grandeza $\rho_{\rvx}(t_1,t_2)$ é uma medida do grau de dependência linear entre as variáveis aleatórias $\rvx_{t_1}$ e $\rvx_{t_2}$, ou seja, quantifica o quanto o diagrama de dispersão de $\rvx_{t_1}$ \emph{versus} $\rvx_{t_2}$ se aproxima de uma reta. Observe-se que a maioria dos autores das áreas de séries temporais e de redes utiliza o termo \ac{FAC} (\ac{ACF}) quando se refere à grandeza definida por (\ref{def:coef-corr}) \cite{leland94}, \cite{paxson95},  \cite{tsay05}, \cite{zivot03}, \cite{morettin04} e \cite{paxson97}. 
%O restante desta tese adota a nomenclatura ``autocorrelação'' para $R_{\rvx}(t_1,t_2)$ e $\rho_{\rvx}(t_1,t_2)$. A notação indicará qual é a autocorrelação em questão. 

\begin{defi}[Estacionariedade em Sentido Amplo\footnote{A nomenclatura ``estacionariedade em sentido amplo'' é amplamente adotada pelos autores da área de engenharia elétrica \cite{gubner06}, \cite{papoulis91}, \cite{stark02}, enquanto que os estatísticos preferem usar os termos ``estacionariedade fraca'' ou ``estacionariedade de segunda ordem'' \cite{tsay05}, \cite{morettin04} e \cite{zivot03}.}]
\label{def:PE-esa}
Um processo aleatório $\rvx_t$ é estacionário em sentido amplo \cite[pág. 298]{papoulis91} se a sua média é constante
\begin{equation}
	\label{eq:esa-media}
	E[\rvx_t] = \mu_{\rvx},
\end{equation}
e se a autocorrelação depende somente do \emph{lag} $\tau=t_2-t_1$:
\begin{equation}
	\label{eq:esa-autocor}
	R_{\rvx}(t_1,t_2) = R_{\rvx}(t_1,t_1 + \tau) = R_{\rvx}(\tau). \quad \blacksquare
\end{equation}
\end{defi}

Observe-se que (\ref{eq:esa-autocor}) implica uma autocovariância que depende do \emph{lag}, ou seja, $C_{\rvx}(\tau)$. Neste caso, a variância do processo estacionário é constante e é dada por 
\begin{equation}
	\label{eq:esa-var}
	\sigma_{\rvx}^2 = C_{\rvx}(0).
\end{equation}
  
%Considere um vetor com $N$ observações de um processo estacionário em sentido amplo. De acordo com a definição \ref{def:PE-esa}, espera-se que essas $N$ observações flutuem com uma variabilidade constante em torno do nível médio $\mu$. 

%Do ponto de vista prático, pode-se dar a seguinte interpretação para a definição \ref{def:PE-esa}. Seja $\dvx$ um vetor com $N$ observações de uma  série temporal estacionária em sentido amplo. De acordo com (\ref{eq:esa-media}) e (\ref{eq:esa-autocor}), as $N$ observações de $x$ devem flutuar com uma variabilidade constante em torno de um nível fixo. 
O restante deste trabalho refere-se aos processos estacionários em sentido amplo simplesmente como ``processos estacionários''. 

O conceito de \textbf{estacionariedade} é fundamental para a análise de séries temporais, dado que a maior parte dos procedimentos de análise estatística supõe que as séries sejam estacionárias. Contudo, séries empíricas podem 
apresentar alguma forma de não-estacionariedade não-explosiva, conforme será explicado a seguir. Portanto, é necessário submeter séries não-estacionárias a algum tipo de transformação que as torne estacionárias. Séries reais geralmente apresentam os seguintes tipos de não-estacionariedade \cite{morettin04}:  
\begin{description}
	\item[(a)] não-estacionariedade quanto ao nível: as séries oscilam ao redor de um nível médio durante algum tempo e depois saltam para outro nível. A primeira diferença torna essas séries estacionárias.
	\item[(b)] não-estacionariedade quanto à inclinação: as séries flutuam ao redor de um reta, com inclinação positiva ou negativa. A segunda diferença desse tipo de série é estacionária.  
\end{description}



%A FAC $\rho(\tau)$ de um processo estacionário é uma medida do grau de dependência linear entre as variáveis aleatórias $\rvx_t$ e $\rvx_{t+\tau}$, ou seja, cada autocorrelação de \emph{lag} $\tau$ quantifica o quanto o diagrama de dispersão de $\rvx_t$ versus $\rvx_{t+\tau}$ se aproxima de uma reta. 
%O valor de $j$ para o qual $\rho(j)=(1/e)=0,37$ é denominado ``tempo de correlação'' $\tau_c$ \cite[pág. 225]{julien03}. Neste sentido, a quantidade $2\tau_c$ pode ser interpretada como uma medida do grau de memória de um modelo. 
%A autocorrelação é a ferramenta básica de estudo dos processos estacionários \cite[pág. 24]{tsay05}. 

\begin{defi}[Ruído Branco Independente]
\label{def:RBI}
Um processo $\rvw_t \sim \text{IID}$ é um \textbf{\ac{RBI}} quando possui média $\mu_{\rvw}$ e variância $\sigma^2_{\rvw}$, $\rvw_t \sim \text{RBI}(\mu_{\rvw},\sigma^2_{\rvw})$. $\quad \blacksquare$
\end{defi}

\begin{defi}[Ruído Branco]
\label{def:RB}
Uma seqüência $\{\rvw_t, t \in \mathbb{Z}\}$ de variáveis aleatórias \textbf{não-correlacionadas}\footnote{Neste caso, $\rho(\tau)=0$ para $\tau \neq 0$.} com média $\mu_{\rvw}$ e variância $\sigma^2_{\rvw}$ é denominada \textbf{\ac{RB}}, $\rvw_t \sim \text{RB}(\mu_{\rvw},\sigma^2_{\rvw})$. $\quad \blacksquare$
\end{defi}

\begin{obs}[Ruído Branco Gaussiano]
\label{obs:RBG}
Um RB Gaussiano (RBG) $\rvw_t$ é um RBI e é denotado por $\rvw_t \sim \mathcal{N}(\mu_{\rvw},\sigma^2_{\rvw})$, em que $\mathcal{N}$ denota a distribuição normal (Gaussiana) de probabilidades. 
\end{obs}


Considere um processo estacionário $\rvx_t$. A média amostral de uma realização $x_t$ com $N$ pontos é dada por
\begin{equation}
	\label{eq:media-amostral}
	\bar{x} = \frac{1}{N}\sum_{t=1}^{N} x_t,
\end{equation} 
a autocovariância amostral de \emph{lag} $\tau$  por
\begin{equation}
	\label{eq:autocov-amostral}
	\hat{C}_{\tau} = \frac{1}{N}\sum_{t=\tau+1}^{N}(x_t - \bar{x})(x_{t-\tau} - \bar{x}),
\end{equation} 
e a autocorrelação amostral (\ac{SACF}) de \emph{lag} $\tau$ por 
\begin{equation}
	\label{eq:autocorr-amostral}
	\hat{\rho}_{\tau} = \frac{\hat{C}_{\tau}}{\hat{C}_0},
\end{equation} 
em que $\hat{C}_0$ (também denotada por $s^2_{\rvx}$)
\begin{equation}
	\label{eq:cov-amostral}
	\hat{C}_{0} = s^2_{\rvx} = \frac{1}{N}\sum_{t=1}^{N}(x_t - \bar{x})^2
\end{equation} 
é a variância amostral\footnote{A variância amostral também pode ser definida como 
$s^2_{\rvx} = \frac{1}{N-1}\sum_{t=1}^{N}(x_t - \bar{x})^2$ \cite[pág.241]{gubner06}.} de $x_t$ \cite[pág.58]{zivot03}.


\begin{defi}[Ergodicidade]
	\label{def:ergodico}
Um processo estacionário $\rvx_t$ é dito ergódico se os seus momentos amostrais convergem em probabilidade\footnote{Diz-se que a seqüência $\{\rvx_1, \rvx_2, \dots, \rvx_n, \ldots\}$ converge em probabilidade para $x$ se $\underset{n\rightarrow\infty} \lim P(|\rvx_n - x|\geq \epsilon)=0$ para todo $\epsilon > 0$.} para os momentos da população; isto é, se $\bar{\rvx} \xrightarrow{p} \mu$, $\hat{C}_{\tau} \xrightarrow{p} C_{\tau}$ e $\hat{\rho}_{\tau} \xrightarrow{p} \rho_{\tau}$ \cite[pág. 58]{zivot03}. $\quad \blacksquare$ 
\end{defi}

%\section{A Construção do Modelo}
\section{Modelagem de Séries Temporais}
\label{sec:mod-st}

A modelagem de uma série temporal $x_t$ consiste na estimação de uma função invertível $h(.)$, denominada \textbf{modelo} de $\rvx_t$, tal que 
\begin{equation} 
	\label{eq:mod-geral}
	\rvx_t = h(\ldots,\rvw_{t-2},\rvw_{t-1},\rvw_{t},\rvw_{t+1},\rvw_{t+2},\ldots),
\end{equation}
em que  $\rvw_t \sim \text{IID}$ e
\begin{equation} 
	\label{eq:mod-geral-inv}
	g(\ldots,\rvx_{t-2},\rvx_{t-1},\rvx_{t},\rvx_{t+1},\rvx_{t+2},\ldots) = \rvw_t,
\end{equation}
em que $g(.)=h^{-1}(.)$. O processo $\rvw_t$ é a \textbf{inovação} no instante $t$ e representa a nova informação sobre a série que é obtida no instante $t$. 

Na prática, o modelo ajustado é \textbf{causal}, ou seja,
\begin{equation} 
	\label{eq:mod-geral-causal}
	\rvx_t = h(\rvw_{t},\rvw_{t-1},\rvw_{t-2},\ldots).
\end{equation}

A metodologia de construção de um modelo é baseada no ciclo iterativo ilustrado pela Fig. \ref{fig:sel-modelo} \cite{box94}, \cite{morettin04}: 
\begin{description}
	\item[(a)] uma classe geral de modelos é considerada para a análise (\textbf{especificação});
	\item[(b)] há a \textbf{identificação} de um modelo, com base em critérios estatísticos;
	\item[(c)] segue-se a fase de \textbf{estimação}, na qual os parâmetros do modelo são obtidos. Na prática, é importante que o modelo seja \textbf{parcimonioso}\footnote{Diz-se que um modelo é parcimonioso quando o mesmo utiliza poucos parâmetros. A utilização de um número excessivo de parâmetros é indesejável porque o grau de incerteza no procedimento de inferência estatística aumenta com o aumento do número de parâmetros.} e 
	\item[(d)] finalmente, há o \textbf{diagnóstico} do modelo ajustado por meio de uma análise estatística da série $w_t$ de resíduos ($w_t$ é compatível com um RB?). 
\end{description}

\begin{figure}
	\centering
		\includegraphics* [keepaspectratio, height=12cm] {figuras/sel-modelo.eps}
	\caption{Ciclo iterativo Box-Jenkins.}
	\label{fig:sel-modelo}
\end{figure} 

%O modelo é dito \textbf{linear} se a função $h(.)$ é linear.
%\begin{equation} 
%	\label{def:mod-linear-geral}
%	g(\ldots,\rvx_{t-2},\rvx_{t-1},\rvx_{t},\rvx_{t+1},\rvx_{t+2},\ldots) = \rvw_t,
%\end{equation}

O processo $\rvx_t$ de (\ref{eq:mod-geral-causal}) é \textbf{linear} quando corresponde à convolução de um processo $\rvw_t \sim  \text{IID}$ com uma seqüência determinística $h_t$ \cite[pág. 377]{taqqu94}
\begin{equation}
\begin{split}
	\label{eq:proc-linear}
	\rvx_t &= h_t \star \rvw_t = \overset{\infty}{\underset{k=0}\sum}h_k \rvw_{t-k} \\
	       &= \rvw_t + h_1\rvw_{t-1} + h_2\rvw_{t-2} + \ldots \\
		     &= (1 + h_1B + h_2B^2 + \ldots)\rvw_t \\
		     &= H(B)\rvw_t \\
\end{split}
\end{equation}
em que o símbolo $\star$ denota a operação de convolução e $h_0=1$. A Eq. (\ref{eq:proc-linear}) também é conhecida como representação de \textbf{média móvel de ordem infinita} (MA$(\infty)$) \cite{brockwell96}.

A forma geral de filtro linear de (\ref{eq:proc-linear}) é\footnote{Demonstra-se que (\ref{eq:proc-linear}) é a única solução do filtro (\ref{eq:arma}) \cite[pág. 377]{taqqu94}, \cite{proakis07}.}
\begin{equation}
	\label{eq:arma}
	\rvx(t) = \overset{p}{\underset{k=1}\sum}\phi_k \rvx(t-k) + \rvw(t) - \overset{q}{\underset{k=1}\sum}\theta_k \rvw(t-k).
\end{equation}
A seqüência $h_t$ é denominada resposta impulsiva de (\ref{eq:arma}), também conhecida como \textbf{modelo ARMA} de ordens $p$ e $q$ (ARMA($p,q$)). 

Numa forma mais compacta, tem-se que 
\begin{equation}
  \label{eq2:arma}
	\phi(B) \rvx_t = \theta(B)\rvw_t, 
\end{equation}	
em que $\phi(B)$ é o operador auto-regressivo de ordem $p$
\begin{equation}
  \label{eq:op-ar}
	\phi(B) = 1 - \phi_1 B - \phi_2 B^2 - \ldots - \phi_p B^p 
\end{equation}
e $\theta(B)$ denota o operador de média móvel de ordem $q$   
\begin{equation}
  \label{eq:op-ma}
	\theta(B) = 1 - \theta_1 B - \theta_2 B^2 - \ldots - \theta_q B^q. 
\end{equation}	

Se todos os pólos e zeros da função de transferência
%\footnote{Demonstra-se que a função de sistema de (\ref{eq:proc-linear}) é racional \cite{proakis07}.}
\begin{equation}
\label{eq:H(z)-proc-linear}
  H(z)=\overset{\infty}{\underset{k=0}\sum} h_k z^{-k} = \frac{\theta(z)}{\phi(z)}
\end{equation}
do filtro (\ref{eq2:arma}) estiverem dentro do círculo de raio unitário
\begin{align}
  \phi(z)   &= 0, \quad |z| < 1 \label{eq:est} \\
  \theta(z) &= 0, \quad |z| < 1 \label{eq:inv}
\end{align}
então o processo $\rvx_t$ dado por (\ref{eq:proc-linear}) é \textbf{estacionário} (ou não-explosivo) e \textbf{invertível}, respectivamente \cite[pág. 537]{brockdavis91}, \cite[pág. 377]{taqqu94}. 

Quando as inovações em (\ref{eq:proc-linear}) são variáveis aleatórias com distribuição $\alpha$-estável\footnote{Uma variável aleatória $X$ com distribuição $\alpha$-estável possui \textbf{cauda pesada}  
e é definida pela seguinte função característica \cite{taqqu94}: 
\begin{equation} 
	\Phi_X(w) = E[e^{jwX}] = \int_{-\infty}^{\infty} f_X(x)e^{jwx}\, dx =
	\text{exp} \{ j\mu w - |\sigma w|^{\alpha} [1-j \eta \,\text{sign}(w) \varphi(w,\alpha)] \},
\end{equation}
em que,
\begin{equation} 
\varphi(w,\alpha)=
	\begin{cases}
		\tan{(\alpha \pi/2)} & \text{se $\alpha\neq1$} \\
		-\frac{2}{\pi}\ln{|w|} & \text{se $\alpha=1$}, 
	\end{cases}
\end{equation}
e $\text{sign}(.)$ denota a função \texttt{sinal}, $\alpha$  ($0<\alpha \leq 2$) é o \textbf{expoente característico}, $\mu$ ($\mu \in \mathbb{R}$) é o \textbf{parâmetro de localização}, $\eta$ ($-1 \leq \eta \leq 1$) é o \textbf{parâmetro de assimetria} e $\sigma \geq 0$ é o \textbf{parâmetro de dispersão} ou \textbf{escala}. A \textbf{variância} de $X$ é \textbf{infinita} para $0<\alpha<2$.}  ($\alpha$-\emph{stable}) \cite{taqqu94}, \cite{feller71}, \cite{nolan:url}, (\ref{eq:proc-linear}) define um \textbf{processo não-Gaussiano com variância infinita} \cite[pág. 535]{brockdavis91}, \cite[pág. 380]{taqqu94}.
Este fato é justificado pelo \textbf{teorema central do limite generalizado} \cite{nolan:url}, \cite{shao93}, o qual afirma que, se o limite de uma soma de variáveis aleatórias IID converge, então este limite só pode ser uma variável aleatória com distribuição estável (a distribuição normal é um caso particular da distribuição estável\footnote{Quando $\alpha=2$.}). O próximo Capítulo introduz os conceitos de variável aleatória com distribuição de cauda pesada e as distribuições estáveis.
%O leitor interessado em obter maiores detalhes sobre processos lineares $\alpha$-estáveis poderá consultar o 
%o apêndice \ref{apend-B} desta tese. 

Se as inovações em (\ref{eq:proc-linear}) tiverem variância finita, ou seja, $\rvw_t \sim \text{RBI}(\mu_{\rvw},\sigma^2_{\rvw})$, então $\rvx_t$ será Gaussiano quando $h_t$ tiver duração infinita (teorema central do limite) \cite[pág. 225]{stark02}). Note-se que qualquer não-linearidade na função $h(.)$ de (\ref{eq:mod-geral-causal}) resulta num processo $\rvx_t$ \textbf{não-linear}. Neste caso, $\rvx_t$ tem estatísticas necessariamente não-Gaussianas. Por outro lado, processos Gaussianos são necessariamente lineares. 
O restante deste Capítulo assume que as inovações em (\ref{eq:proc-linear}) são do tipo  $\rvw_t \sim \text{RBI}(\mu_{\rvw},\sigma^2_{\rvw})$.  


%Um processo $\rvx_t$ é \textbf{linear} com variância finita \cite{tsay05}, \cite{baccala03} 
%\begin{equation}
%\begin{split}
%	\label{eq2:proc-linear}
	%\tilde{\rvx}_t &= \rvx_t - \mu \\
%	\rvx_t = \overset{\infty}{\underset{k=0}\sum}h_k \rvw_{t-k}, %\\
%		             &= \rvw_t + h_1\rvw_{t-1} + h_2\rvw_{t-2} + \ldots = H(B)\rvw_t, \\
%\end{split}
%\end{equation}


%A modelagem linear de processos aleatórios está baseada no \textbf{modelo de filtro linear} ilustrado pela Fig.\ref{fig:proc-linear-geral}. Este modelo assume que o processo $\rvx_t = \tilde{\rvx}_t + \mu$, em que $\mu$ é um parâmetro que determina o nível do modelo, seja gerado por meio da filtragem linear (causal) de um processo $\rvw_t \sim \text{RBI}(0,\sigma^2_{\rvw})$: 
%\begin{equation}
%	 \rvw_t \sim (0,\sigma^2_w).  
%\end{equation}

%Reescrevendo-se (\ref{eq:proc-linear})
%\begin{equation}
%\begin{split}
%	\label{eq2:proc-linear}
%	\rvx_t  &= \rvw_t + h_1\rvw_{t-1} + h_2\rvw_{t-2} + \ldots, \\
%		      &= H(B)\rvw_t, \\
%\end{split}
%\end{equation}
%tem-se que o operador 
%\begin{equation}
%	\label{eq:func-transf-B}
%	H(B) = 1 + h_1B + h_2B^2 + \ldots,
%\end{equation}
%ou a sua versão na variável $z$ 
%\begin{equation}
%	\label{eq:func-transf-z}
%	H(z) = 1 + h_1z^{-1} + h_2z^{-2} + \ldots,
%\end{equation}
%denota a função de transferência do filtro. 

%\begin{figure}
%	\centering
%		\includegraphics[height=3cm,keepaspectratio]{figuras/proc-linear-geral.eps}
%	\caption{Modelo de filtro linear.}
%	\label{fig:proc-linear-geral}
%\end{figure} 

%O filtro (\ref{eq:func-transf-z}) é  \textbf{estável}\footnote{No sentido \ac{BIBO}\cite[pág. 29]{opp89}.} \cite[pág. 111]{morettin04}, \cite[pág. 29]{opp89} quando a sua resposta impulsiva é somável em módulo
%\begin{equation}
%	\label{eq:BIBO}
%	\overset{\infty}{\underset{t=0}\sum} |h_t| = K < \infty. 
%\end{equation}
%Neste caso, $\rvx_t$ é estacionário com média $\mu$. Caso contrário, $\rvx_t$ é não-estacionário e $\mu$ não tem  significado específico. A condição (\ref{eq:BIBO}) implica que $H(z)$ deve convergir sobre o círculo de raio unitário $|z|= 1$ \cite[pág. 114]{morettin04}, \cite[pág. 50]{box94}\footnote{Esta condição garante a existência da \ac{TFTD} de $h_t$ definida por $\overset{\infty}{\underset{t=-\infty}\sum}h_t$ \cite{opp89}.}. 

%De acordo com a literatura da área de econometria \cite{hamilton94} \emph{apud} \cite[pág. 64]{zivot03}, 

%O processo linear $\rvx_t$ dado por (\ref{eq:proc-linear}) é \textbf{estacionário} e \textbf{ergódico} com média $\mu$ quando a sua resposta impulsiva $h_t$ obedece à condição de ``somabilidade-1'' (1-\emph{summability}) \cite{hamilton94} \emph{apud} \cite[pág. 64]{zivot03}
%\begin{equation}
%	\label{eq:somab-1}
%	\overset{\infty}{\underset{t=0}\sum} t|h_t| = 1 + 2|h_2|+ 3|h_3| + \ldots < \infty. 
%\end{equation}
%Caso contrário, $\rvx_t$ é \textbf{não-estacionário}. 

%A existência da \ac{TFTD} de $h_t$ \cite{opp89}
%\begin{equation}
%	\label{eq:TFTD}
%  H(e^{jw}) = \overset{\infty}{\underset{t=-\infty}\sum}h_t
%\end{equation}

%é válida, garante-se que $H(z)$ converge sobre o círculo de raio unitário $|z|= 1$ \cite[pág. 114]{morettin04}, \cite[pág. 50]{box94}. Portanto, a existência da \ac{TFTD} de $h_t$ definida por $\overset{\infty}{\underset{t=-\infty}\sum}h_t$ \cite{opp89}.}. 

%O modelo (\ref{eq:proc-linear}) é \textbf{invertível} quando a série
%\begin{equation}
%	\label{eq:p-pi}
%	G(z) = H^{-1}(z) 
%\end{equation} 
%converge para $|z|\geq 1$ \cite[pág. 50]{box94}. Neste caso, o modelo inverso de (\ref{eq:proc-linear}) é dado por
%\begin{equation}
%  \label{eq:modelo-inv}
%	\rvw_{t} = G(B)\rvx_t. 
%\end{equation}
%O operador $G(B)$ é definido como
%\begin{equation}
%	\label{eq:oper-pi}
%	G(B)= 1 -g_1B - g_2 B^2 - \dots 
%\end{equation}

%A condição de invertibilidade é independente da condição de estacionariedade\footnote{A convergência da série $H(z)$ para $|z|\geq 1$ é uma condição de estacionariedade alternativa à condição de somabilidade-1 (\ref{eq:somab-1}) \cite{box94}. Não obstante, as duas condições garantem a existência da transformada de Fourier de $h_t$.}, sendo também aplicável aos modelos lineares não-estacionários da classe ARIMA \cite[pág. 50]{box94}. 

A autocorrelação do RBI $\rvw_t$ em (\ref{eq:proc-linear}) é
\begin{equation}
	\label{eq:corr-w}
	R_\rvw(\tau) = \sigma^2_{\rvw} \delta_\tau + \mu^2_{\rvw}, 
\end{equation}
em que $\delta_\tau$ denota o pulso unitário de tempo discreto\footnote{$\delta_\tau=1$ para $\tau=0$, $\delta_\tau=0$ para $\tau\neq0$, $\tau \in \mathbb{Z}$.}. Logo a sua função densidade espectral ou \ac{DEP}\footnote{A DEP de $\rvw_t$ é definida como a \ac{TFTD} da sua autocorrelação $R_\rvw(\tau)$, ou seja, $S_{\rvw}(f) = \overset{\infty}{\underset{m=-\infty}\sum} R_\rvw(m) e^{-j2\pi fm}$ \cite[pág. 351]{stark02}.} é
\begin{equation}
	\label{eq:DEP-w}
	S_{\rvw}(f) = \mu_{\rvw}^2\delta(f) + \sigma^2_\rvw,\quad -1/2 \leq f \leq 1/2,
\end{equation}
em que $f$ denota a freqüência normalizada e $\delta(f)$ é a função generalizada Impulso (ou Delta de Dirac).

A DEP do processo linear $\rvx_t$ é \cite{opp89}
\begin{equation}
	\label{eq:DEP-x}
\begin{split}
	S_{\rvx}(f) &= |H(f)|^2 S_{\rvw}(f) \\
	            &= \mu_{\rvw}^2|H(0)|^2\delta(f) + \sigma^2_\rvw |H(f)|^2,\quad -1/2 \leq f \leq 1/2,
\end{split}
\end{equation}
em que $H(f) = H(z)|_{z=e^{j2\pi f}}$ é a resposta em freqüência do filtro.

A autocovariância de $\rvx_t$ é dada por \cite[pág. 112]{morettin04}
\begin{equation}
	\label{eq:facv-x}
	C_\rvx(\tau) = \sigma^2_\rvw\overset{\infty}{\underset{t=0}\sum} h_t h_{t+\tau},
\end{equation}
e a sua variância por
\begin{equation}
	\label{eq:var-x}
	\sigma^2_{\rvx} = \sigma^2_\rvw \overset{\infty}{\underset{t=0}\sum} h_t^2.
\end{equation}

O restante desta tese assume, sem perda de generalidade, que a média do processo linear $\rvx_t$ de (\ref{eq:proc-linear}) seja nula ($\mu_{\rvx}=0$).   

Como deseja-se que os modelos estimados na prática sejam invertíveis (ou seja, a condição (\ref{eq:inv}) é válida), pode-se definir o operador inverso $G(B)=H^{-1}(B)$ e reescrever (\ref{eq:proc-linear}) na forma auto-regressiva de ordem infinita (AR($\infty$))
\begin{equation}
\begin{split}
	\label{eq:modelo-ar-inf}
	\rvx_t &= g_1 \rvx_{t-1} + g_2 \rvx_{t-2} + \ldots + \rvw_t \\
				 &= \overset{\infty}{\underset{k=1}\sum} g_k \rvx_{t-k} + \rvw_t. \\ 
\end{split}	  
\end{equation}
Portanto, $\rvx_t$ pode ser interpretado como uma soma ponderada de seus valores passados $x_{t-1}, x_{t-2},\ldots$ 
mais uma inovação $\rvw_t$. O modelo equivalente AR($\infty$) sugere que pode-se calcular a probabilidade de um valor futuro $\rvx_{t+k}$ estar entre dois limites especificados, ou seja, (\ref{eq:modelo-ar-inf}) afirma que é possível fazer inferências ou \textbf{previsões} de valores futuros da série.

\section{Modelos Auto-Regressivos}

Um modelo auto-regressivo de ordem $p$ (AR($p$)) satisfaz à equação
\begin{equation}
	\label{eq:modelo-ar}
	\phi(B) \rvx_t = \rvw_t. 
\end{equation}

%\subsection{Estacionariedade e Invertibilidade de Modelos AR}
%O modelo AR($p$) é estacionário (e causal) se todos os pólos da função de sistema $H(z)=1/\phi(z)$ estão dentro do círculo de raio unitário\footnote{A condição equivalente na variável complexa $B=z^{-1}$ exige que todas as raízes de $\phi(B)=0$ estejam fora do círculo de raio unitário $|B|=1$.} $|z|=1$. Como $\pi(B) = \phi(B)$ é finito, o modelo AR($p$) sempre é invertível.   
  
\subsection{Função de Autocorrelação}
Multiplicando-se ambos os membros de (\ref{eq:modelo-ar}) por $\rvx_{t-k}$ e tomando-se a esperança obtém-se
\begin{equation*}
E[\rvx_t \rvx_{t-k}] = \phi_1 E[\rvx_{t-1}\rvx_{t-k}] + \phi_2 E[\rvx_{t-2}\rvx_{t-k}] + \ldots + 
\phi_p E[\rvx_{t-p}\rvx_{t-k}] + E[\rvw_t\rvx_{t-k}], 
\end{equation*}
como $\rvx_{t-k}$ não depende de $\rvw_t$, mas somente de ruídos até o instante $t-k$, que são não-correlacionados com $\rvw_t$, então $E[\rvw_t\rvx_{t-k}]=0$, $k>0$, e
\begin{equation}
\label{eq:cov-modelo-ar}
C_{\rvx}(k) = \phi_1 C_{\rvx}(k-1) + \phi_2 C_{\rvx}(k-2) + \ldots + \phi_p C_{\rvx}(k-p),\quad k>0. 
\end{equation}
Dividindo-se (\ref{eq:cov-modelo-ar}) por $C_{\rvx}(0)=\sigma_{\rvx}^2$, obtém-se  
\begin{equation}
\label{eq:fac-modelo-ar}
\rho_{\rvx}(k) = \phi_1 \rho_{\rvx}(k-1) + \phi_2 \rho_{\rvx}(k-2) + \ldots + \phi_p \rho_{\rvx}(k-p),\quad k>0, 
\end{equation}
ou
\begin{equation}
\label{eq2:fac-modelo-ar}
\phi(B)\rho_{\rvx}(k) =0. 
\end{equation}

Sejam $G_i^{-1}$, $i=1,\ldots,p$, as raízes de $\phi(B)=0$ (\textbf{equação característica} do modelo \cite[pág. 40]{tsay05}). Então pode-se escrever que
\begin{equation*}
\phi(B) = \overset{p}{\underset{i=1}\prod} (1-G_iB),
\end{equation*}
e demonstra-se que a solução geral de (\ref{eq2:fac-modelo-ar}) é \cite[pág. 116]{morettin04} 
\begin{equation}
\label{eq:sol-geral-fac-ar}
\rho_{\rvx}(k) = A_1G_1^k + A_2G_2^k + \dots + A_pG_p^k, 
\end{equation}
em que as constantes $A_i$, $i=1,2,\ldots,p$, são determinadas\footnote{Por exemplo, considere um modelo AR(2)
\[ \rvx_t = \phi_1 \rvx_{t-1} + \phi_2 \rvx_{t-2} + \rvw_t\] 
e a sua FAC, a qual satisfaz a equação de diferenças de segunda ordem
\[ \rho_k = \phi_1 \rho_{k-1} + \phi_2 \rho_{k-2}, \quad k>0 \]
com valores iniciais $\rho_{0}=1$ e  $\rho_{1}=\frac{\phi_1}{1-\phi_2}$. De (\ref{eq:sol-geral-fac-ar}), tem-se que a solução geral dessa Eq. de diferenças de segunda ordem é \cite[pág.59]{box94}
\[ \rho_k = \frac{G_1 (1 - G_2^2)G_1^k - G_2(1-G^2_1)G_2^k}{(G_1-G_2)(1+G_1G_2)}.\]
} por condições iniciais sobre $\rho_{\rvx}(0), \rho_{\rvx}(1),\ldots,\rho_{\rvx}(p-1)$. Como as raízes da equação característica do modelo devem estar fora do círculo unitário\footnote{Como $B=z^{-1}$, então as Eqs. 
\ref{eq:est} e \ref{eq:inv} podem ser reescritas como $\phi(B)=0$ para $|B|>1$ e $\theta(B)=0$ para $|B|>1$.}, deve-se ter que $|G_k|<1$, $k=1,\ldots,p$. Portanto, o gráfico da FAC de um processo AR($p$) é, em geral, constituído de uma mistura de exponenciais e senóides amortecidas \cite[pág. 40]{tsay05}. 


\subsection{Identificação}
Na prática, a ordem $p$ de uma série AR é desconhecida e deve ser especificada de forma empírica. Há duas abordagens para se determinar o valor de $p$: i) uso da \textbf{\ac{FACP}} e ii) uso de algum \textbf{critério de seleção (identificação) de modelo}\cite{tsay05}.

\subsubsection{FACP de modelos AR($p$)}

Seja $\phi_{mi}$ o i-ésimo coeficiente de um processo AR($m$), de modo que o último coeficiente seja $\phi_{mm}$.
Fazendo-se $k=1,\ldots,m$ em (\ref{eq:fac-modelo-ar}) (adota-se a seguir a notação simplificada $\rho_{\rvx}(k)=\rho_{k}$) e levando-se em conta que $\rho_{k}=\rho_{-k}$ (simetria par da FAC), obtêm-se as \textbf{equações de Yule-Walker} \cite[pág.64]{box94}, \cite[pág.134]{morettin04}, \cite[pág.69]{zivot03}
\begin{equation}
\begin{split}
\label{eq:yule-walker}
\rho_1 &= \phi_{m1} + \phi_{m2} \rho_1 + \ldots + \phi_{mm} \rho_{m-1},\\
\rho_2 &= \phi_{m1}\rho_1 + \phi_{m2} + \ldots + \phi_{mm} \rho_{m-2},\\
               & \vdots \\
\rho_m &= \phi_{m1}\rho_{m-1} + \phi_{m2}\rho_{m-2} + \ldots + \phi_{mm},\\
\end{split}
\end{equation}
que podem ser reescritas em forma matricial
\begin{gather}
		\begin{bmatrix}
		1 & \rho_1 & \ldots & \rho_{m-1} \\
		\rho_1 & 1 & \ldots & \rho_{m-2} \\
		\vdots         & \vdots  &  \hdots      & \vdots\\
		\rho_{m-1} & \rho_{m-2} & \ldots & 1 
		\end{bmatrix} 
		\begin{bmatrix}
		\phi_{m1} \\ 
		\phi_{m2} \\
		\vdots \\
		\phi_{mm} \\
		\end{bmatrix} =
    \begin{bmatrix}
    \rho_1 \\ 
		\rho_2 \\
		\vdots \\
		\rho_m \\
    \end{bmatrix}
\end{gather} 		
ou na forma compacta
\begin{equation}
	\label{eq:YW-compacta}
	R_m\phi_m = \tilde{\rho}_m,
\end{equation}
em que $R_m$ denota a matriz de autocorrelações de ordem $m$, $\phi_m$ é o vetor de parâmetros do modelo e $\tilde{\rho}_m$ é o vetor de autocorrelações.

Resolvendo-se (\ref{eq:YW-compacta}) sucessivamente para $m=1,2,\ldots$, obtém-se
\begin{equation}
\begin{split}
	\phi_{11} &= \rho_{1} \\
	\phi_{22} &= \frac{ \begin{vmatrix} 1 & \rho_1 \\ \rho_1 & \rho_2 \end{vmatrix} }
	                 { \begin{vmatrix} 1 & \rho_1 \\ \rho_1 & 1 \end{vmatrix}} \\
	\phi_{33} &= \frac{ \begin{vmatrix} 
	                    1      & \rho_1 & \rho_1 \\ 
	                    \rho_1 & 1      & \rho_2  \\
	                    \rho_2 & \rho_1 & \rho_3    
	                    \end{vmatrix} }
	                  { \begin{vmatrix} 
	                    1      & \rho_1 & \rho_2 \\ 
	                    \rho_1 & 1      & \rho_1  \\
	                    \rho_2 & \rho_1 & 1  
	                    \end{vmatrix}} \\
\end{split}
\end{equation}
e, em geral,
\begin{equation}
	\phi_{mm} = \frac{|R_m^{\ast}|}{|R_m|},
\end{equation}
em que $|R_m|$ denota o determinante da matriz de autocorrelações de ordem $m$ e $R_m^{\ast}$ é a matriz $R_m$ com a última coluna substituída pelo vetor de autocorrelações. A seqüência $\{\phi_{mm}, m=1,2,\ldots\}$ é a FACP. Demonstra-se que um modelo AR($p$) tem $\phi_{mm} \neq 0$ para $m \leq p$ e $\phi_{mm} = 0$ para $m > p$ \cite{box94}. 

A FACP pode ser estimada por meio do ajuste da seqüência de modelos AR($m$), $m=1,2,\ldots$ 
\begin{equation}
\begin{split}
\label{eq:facp-ar}
\rvx_t &= \phi_{11}\rvx_{t-1} + \rvw_{1t}\\
\rvx_t &= \phi_{21}\rvx_{t-1} + \phi_{22}\rvx_{t-2} + \rvw_{2t}\\
               & \vdots \\
\rvx_t &= \phi_{m1}\rvx_{t-1} + \phi_{m2}\rvx_{t-2} + \ldots + \phi_{mm}\rvx_{t-m} + \rvw_{mt},\\
               & \vdots \\
\end{split}
\end{equation}
pelo método dos mínimos quadrados \cite[pág. 40]{tsay05}, \cite[pág. 70]{zivot03}. A seqüência $\{\hat{\phi}_{mm}, m=1,2,\ldots\}$ obtida é a \ac{FACPA}.

\subsubsection{Identificação do Modelo}
A idéia básica de um critério de seleção (ou critério de informação) de modelo ARMA é escolher as ordens $k$ e $l$ que minimizam a quantidade \cite{morettin06}
\begin{equation}
	\label{eq:crit-id}
	P(k,l) = \ln{\hat{\sigma}^2_{k,l}} + (k+l)\frac{C(N)}{N},
\end{equation}
em que $\hat{\sigma}^2_{k,l}$ é uma estimativa da variância residual obtida ajustando-se um modelo ARMA($k,l$) às $N$ observações da série e $C(N)$ é uma função do tamanho da série. A quantidade  $(k+l)\frac{C(N)}{N}$ é denominada termo penalizador e aumenta quando o número de parâmetros aumenta, enquanto que $\hat{\sigma}^2_{k,l}$ diminui.

Akaike \cite{akaike73}, \cite{akaike74} propôs o critério de informação
\begin{equation}
	\label{eq:AIC}
	AIC(k,l) = \ln{\hat{\sigma}^2_{k,l}} + \frac{2(k+l)}{N},
\end{equation}
conhecido como \ac{AIC}, em que $\hat{\sigma}^2_{k,l}$ é o estimador de máxima verossimilhança de $\sigma^2_{\rvw}$ para um modelo ARMA($k,l$). Deve-se especificar valores limites superiores $K$ e $L$ para $k$ e $l$ e calcular (\ref{eq:AIC}) para todas as possíveis combinações $(k,l)$ com $0\leq k \leq K$ e $0\leq l \leq L$. Em geral, $K$ e $L$ são funções de $N$, por exemplo, $K=L=\ln N$ \cite[pág. 85]{morettin06}. 

Para o caso de modelos AR($p$), (\ref{eq:AIC}) reduz-se a
\begin{equation}
	\label{eq:AIC-ar}
	AIC(k) = \ln{\hat{\sigma}^2_{k}} + \frac{2k}{N}, \quad k \leq K.
\end{equation}
 
Outro critério sistemático bastante utilizado é o (Schwarz) \ac{BIC} \cite[pág. 77]{zivot03}
\begin{equation}
	\label{eq:BIC}
	BIC(k,l) = \ln{\hat{\sigma}^2_{k,l}} + \frac{\ln N}{N}(k+l).
\end{equation}

Para o caso de modelos AR($p$), (\ref{eq:BIC}) reduz-se a
\begin{equation}
	\label{eq:BIC-ar}
	BIC(k) = \ln{\hat{\sigma}^2_{k}} + \frac{k\ln N}{N}.
\end{equation}


\subsection{Estimação de Modelos AR}

Tendo-se identificado a ordem $p$ do modelo AR, pode-se partir para a fase de estimação dos parâmetros. Os métodos dos momentos, \ac{MQ} e \ac{MV} podem ser usados para tal \cite{morettin04}, \cite{percival93}, \cite{morettin06}. Como em geral os estimadores de momentos não são bons \cite{morettin04}, pacotes estatísticos como \texttt{S-PLUS}, \texttt{E-VIEWS}, etc., utilizam algum \ac{EMQ} ou \ac{EMV}. Os livros \cite{morettin04} e \cite{morettin06} de Morettin fornecem maiores detalhes sobre o assunto.  

%\subsection{\emph{Goodness of Fit}}

\section{Modelagem de Séries Não-Estacionárias}
Um processo não-estacionário tem momentos que dependem do tempo. As formas mais comuns de não-estacionariedade são causadas pela variação da média e da variância com o tempo.

\subsection{Séries Estacionárias com Relação a Tendências}
Um processo $\rvy_t$ é dito estacionário com relação a tendências se for da forma
\begin{equation}
	\label{eq:TSP}
	\rvy_t = TD_t + \rvx_t,
\end{equation}
em que $TD_t$ denota o termo de tendências determinísticas (constante, tendência, sazonalidade, etc.) que depende de $t$ e $\rvx_t$ é um processo estacionário. Por exemplo, o processo $\rvy_t$ dado por
\begin{equation}
\begin{split}
\label{eq:TSP-AR1}
	\rvy_t &= \mu + \delta t + \rvx_t,\quad \rvx_t = \phi\rvx_{t-1} + \rvw_t \\
	\rvy_t - \mu - \delta t &=   \phi(\rvy_{t-1} - \mu - \delta (t-1) )+ \rvw_t \\
	\rvy_t  &= c + \beta t + \phi\rvy_{t-1} + \rvw_t \\
\end{split}
\end{equation}
em que $|\phi|<1$, $c=\mu(1-\phi)+\delta$, $\beta=\delta(1-\phi)t$ e $\rvw_t$ é um RBG com média nula e potência $\sigma^2$, é um processo AR(1) estacionário com relação a tendências \cite{zivot03}. 

\subsection{Modelo ARIMA}
\label{sec:arima}

Se o processo que corresponde à diferença de ordem $d=1,2,\ldots$ de $\rvx_t$
\begin{equation}
	\label{eq:dif}
	\rvy_t= (1-B)^d \rvx_t = \Delta^d \rvx_t
\end{equation}
for estacionário, então pode-se representar $\rvy_t$ por meio de um modelo ARMA($p,q$)
\begin{equation}
	\label{eq:arma-y(t)}
	\phi(B) \rvy_t = \theta(B)\rvw_t.
\end{equation}
Neste caso, 
\begin{equation}
	\label{eq:arima}
	\phi(B)\Delta^d \rvx_t = \theta(B)\rvw_t
\end{equation}
é um modelo ARIMA($p,d,q$) e diz-se que $\rvx_t$ é uma ``integral'' de $\rvy_t$ \cite{morettin04}, pois
\begin{equation}
	\label{eq:som}
	\rvx_t= S^d \rvy_t.
\end{equation}
É daí que surge o termo ``integrado'' do acrônimo ARIMA, indicando que (\ref{eq:arima}) é um modelo integrado de ordem $d$, denotado por $\rvx_t \sim I(d)$.  

Como o filtro ARIMA($p,d,q$) 
\begin{equation}
	\label{eq:filtro-arima}
	H(z)= \frac{\theta(z)}{\phi(z)(1-z^{-1})^d}
\end{equation}
é marginalmente estável \cite{proakis07}, pois possui $d$ pólos sobre o círculo unitário, $\rvx_t$ de (\ref{eq:arima}) é um processo \textbf{não-estacionário homogêneo} (no sentido de ser \textbf{não-explosivo}) ou portador de \textbf{raízes unitárias} \cite{tsay05}, \cite{morettin04}, \cite{zivot03}. Observe-se que \cite[pág.139]{morettin04}: 
\begin{description}
	\item[(a)] $d=1$ corresponde ao caso de séries não-estacionárias homogêneas quanto ao nível (oscilam ao redor de um nível médio durante algum tempo e depois saltam para outro nível temporário);
	\item[(b)] $d=2$ corresponde ao caso de séries não-estacionárias homogêneas quanto à inclinação (oscilam numa direção por algum tempo e depois mudam para outra direção temporária).
\end{description}	

O modelo ARIMA (\ref{eq:arima}) pode ser representado de três formas:
\begin{description}
	\item[(a)] ARMA($p+d,q$) (similar à Eq. (\ref{eq:arma})) 
	           \begin{equation}
	           	\label{eq:arima-eqdif}
	           	\rvx(t) = \overset{p+d}{\underset{k=1}\sum}\varphi_k \rvx(t-k) + \rvw(t) -                                                           \overset{q}{\underset{k=1}\sum}\theta_k \rvw(t-k);
	           \end{equation}
	           
	\item[(b)] AR($\infty$) (forma invertida), dada por (\ref{eq:modelo-ar-inf}) ou
	\item[(c)] MA($\infty$), conforme (\ref{eq:proc-linear}).
\end{description}

\subsection{Passeio Aleatório}

Considere o modelo $\rvy_t \sim I(1)$ 
\begin{equation}
	\label{eq:RW}
	\rvy_t = \rvy_{t-1} + \rvx_t,
\end{equation}
em que $\rvx_t$ é um processo estacionário. Supondo-se a condição inicial $y_0$, tem-se que (\ref{eq:RW}) pode ser reescrita como uma soma integrada
\begin{equation}
	\label{eq:RW-IntSum}
	\rvy_t = \rvy_0 + \sum_{j=1}^{t} \rvx_j.
\end{equation}
A soma integrada $\sum_{j=1}^{t} \rvx_j$ é denominada tendência estocástica e é denotada por $TS_t$. Observe-se que
\begin{equation}
	\label{eq:RW2}
	TS_t = TS_{t-1} + \rvx_t,
\end{equation}
em que $TS_0=0$.

Se $\rvx_t \sim \mathcal{N}(0,\sigma^2_{\rvx})$ em (\ref{eq:RW}), então $\rvy_t$ é conhecido como \textbf{passeio aleatório} ou \textbf{casual}. Incluindo-se uma constante no lado direito de (\ref{eq:RW}), tem-se um passeio aleatório com \emph{drift},
\begin{equation}
	\label{eq:RW-drift}
	\rvy_t = \theta_0 + \rvy_{t-1} + \rvx_t.
\end{equation}

Dada a condição inicial $y_0$, pode-se escrever
\begin{equation}
\begin{split}
\label{eq2:RW-drift}
	\rvy_t &= y_0 + \theta_0 t + \sum_{j=1}^{t}\rvx_{j} \\
	       &= TD_t + TS_t 
\end{split}
\end{equation}
e demonstra-se que a média, variância, autocovariância e ACF de $\rvy_t$ são dadas respectivamente por \cite{morettin03}
\begin{align}
	\mu_{t}             &= y_0 + t \theta_0 \\
	\sigma^2(t)            &= t \sigma^2_{\rvx} \\
	C_{k}(t)            &= (t-k)\sigma^2_{\rvx}\\
	\rho_{k}(t)         &= \frac{t-k}{t}.  
\end{align}
Observe que $\rho_{k}(t)\approx 1$ quando $t>>k$ e é por isso que a literatura afirma que um passeio aleatório tem ``\emph{strong memory}'' (``memória forte'') \cite{tsay05}. Note também que a \ac{SACF} do passeio aleatório decai linearmente para grandes \emph{lags}.  
 