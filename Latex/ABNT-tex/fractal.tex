\chapter{A Natureza Fractal do Teletráfego}
\label{cap:fractal}
% Redação concluída em 04/12/07

Este Capítulo introduz a noção de fractal e formula os conceitos de LRD e auto-similaridade de segunda ordem. 
Também são abordados os conceitos de variável aleatória com distribuição de cauda pesada, distribuições estáveis e impulsividade \cite{athina04}. Uma discussão detalhada dos conceitos de fractais e multifractais não faz parte do escopo desta tese. Recomenda-se ao leitor interessado a leitura das seguintes referências: \cite{beran94}, \cite{mandelbrot77a}, \cite{falconer90}, \cite{feldmann98}, \cite{park00}, \cite{abry00}, \cite{riedi99}, \cite{riedi03} e \cite{julien03}.   

\section{Fractais}
\label{fractal:intro}

As formas da geometria clássica - triângulos, círculos, esferas, etc. - perdem suas estruturas quando ampliadas (isto é, quando se dá um \emph{zoom} numa determinada região da figura). Por exemplo, uma pessoa na superfície da Terra tem a impressão de que a mesma é plana (já um astronauta em órbita vê uma Terra redonda). 
Suponha que um indivíduo não tenha sido informado de que foi posicionado num ponto de um círculo com raio de curvatura da ordem de centenas de quilômetros. Esse observador percebe o círculo como sendo uma reta, apesar disto não ser verdade. 

Benoit B. Mandelbrot propôs em 1975 \cite{mandelbrot77a} o termo \textbf{fractal} (do latim \emph{fractus}, que tem o significado de fraturado, quebrado) para designar objetos matemáticos que possuem uma estrutura rica em detalhes ao longo de muitas escalas de observação. O conjunto de Mandelbrot ou o ``boneco de pão-de-mel'' da Fig. \ref{fig:mandelbrot-set} é um fractal matemático com estrutura detalhada (ou seja, é altamente irregular) ao longo de uma série infinita de escalas. Note-se que a Fig. \ref {fig:mandelbrot-set} \subref{fig:mandelbrot-set3}, obtida por meio de um \emph{zoom} de uma determinada região da Fig. \ref{fig:mandelbrot-set} \subref{fig:mandelbrot-set2}, mostra que existe um sub-boneco de pão-de-mel embebido no boneco original. Esta característica ilustra uma outra propriedade dos fractais conhecida como \textbf{auto-similaridade}: um objeto auto-similar contém réplicas menores de si mesmo em todas as escalas. O conjunto de Mandelbrot é um exemplo de fractal determinístico, pois é exatamente auto-similar.

\begin{figure}%
\centering
		\subfloat[]{%
			\label{fig:mandelbrot-set2}%[height=5cm,keepaspectratio]
			\includegraphics{figuras/paodemel1.eps}}%		
		\hspace{-1pt}%
		\subfloat[]{%
			\label{fig:mandelbrot-set3}%
			\includegraphics{figuras/paodemel2.eps}}\\
\caption[]{
A Fig. \subref{fig:mandelbrot-set2} é o conjunto de Mandelbrot, também conhecido como ``boneco de pão-de-mel''. A Fig \subref{fig:mandelbrot-set3} mostra um sub-boneco de pão-de-mel que está embebido na Fig. \subref{fig:mandelbrot-set2}. As figuras foram obtidas via \texttt{MATLAB} utilizando-se o código de A. Klimke \cite{andreas03}, da Universidade de Stuttgart.}%
\label{fig:mandelbrot-set}%
\end{figure}  

%É bastante conhecida a seguinte frase de Mandelbrot (tradução livre) \cite{mandelbrot77b} :
%\begin{quote}
%Nuvens não são esferas, montanhas não são cones, os litorais não são círculos, a casca das árvores não é lisa e tampouco a luz viaja em linha reta.
%\end{quote}

\begin{figure}%
\centering
		\subfloat[]{%
			\label{fig:lit1}%[height=5cm,keepaspectratio]
			\includegraphics[height=4.3cm,width=6.1cm]{figuras/Sweden1.eps}}%		
		\hspace{10pt}%
		\subfloat[]{%
			\label{fig:lit2}%
			\includegraphics[height=4.3cm,width=6.1cm]{figuras/Sweden2.eps}} \\
		\subfloat[]{%
			\label{fig:lit3}%
			\includegraphics[height=4.3cm,width=6.1cm]{figuras/Sweden3.eps}}%
\caption[]{
As Figs. \subref{fig:lit1}, \subref{fig:lit2} e \subref{fig:lit3} são fotos do litoral da Suécia (com resoluções de 1,4 km, 500 m e 250 m, respectivamente) que foram tiradas pelo satélite Terra em 2000 \cite{frame06}. Essas fotos mostram que o contorno de um litoral é fractal: quando ampliado, novos detalhes são revelados.}%
\label{fig:litoral}%
\end{figure}  

\begin{figure}
	\begin{center}
		\includegraphics[height=4.3cm,width=6.1cm]{figuras/couveflor.eps}
	\end{center}
	\caption{Foto que ilustra a auto-similaridade de uma couve-flor \cite{frame06}. Note que as partes menores se parecem com a couve-flor original.}
\label{fig:couveflor}
\end{figure}  

As ciências físicas e humanas nos dão vários exemplos de fractais aleatórios, em que a auto-similaridade ocorre num sentido estatístico, tais como séries temporais nas áreas de climatologia e hidrologia, séries de imagens de ressonância magnética funcional do cérebro humano, movimento turbulento de fluidos e séries de dados financeiros, dentre outros, que envolvem uma extensa faixa de escalas de medição \cite{beran94}, \cite{percival00}, ou seja, que não ocorrem numa escala temporal ou espacial característica. O contorno de um litoral (vide Fig. \ref{fig:litoral}) e a couve-flor ({vide Fig. \ref{fig:couveflor}) são exemplos de fractais encontrados na natureza \cite{frame06}. 

Um fractal matemático bastante conhecido é o conjunto de Cantor, descrito em 1883 pelo matemático alemão Georg Cantor \cite{cantor1883} apud \cite{peitgen92}, o qual é constituído por um número infinito não-enumerável de pontos no segmento $[0,1]$ da reta real. O conjunto de Cantor pode ser obtido de acordo com os seguintes passos: 1) comece com o intervalo $[0,1]$, 2) remova o intervalo aberto $(1/3,2/3)$, ou seja, remova o terço do meio de $[0,1]$, mas não os números $1/3$ e $2/3$ e 3) a cada nova iteração, remova o terço do meio (intervalo aberto) dos segmentos resultantes do passo anterior. A Fig. \ref{fig:cantor} mostra as cinco primeiras iterações do procedimento acima descrito.  
\begin{figure}
	\begin{center}
		\includegraphics[height=5cm,keepaspectratio]{figuras/cantor.eps}
	\end{center}
	\caption{Ilustração das cinco primeiras iterações para obtenção do conjunto de Cantor. Repare-se que o formato de uma determinada parte do objeto, quando magnificada, se assemelha ao formato do todo (auto-similaridade).}
	\label{fig:cantor}
\end{figure}

%Os fenômenos físicos geralmente ocorrem em alguma escala espacial ou temporal característica (ou natural) de observação. A mecânica quântica, por exemplo, descreve a estrutura da matéria em escala atômica. Já a estrutura do universo é bem descrita em escalas de comprimento de milhões de anos-luz. Por outro lado, as ciências físicas e humanas nos dão vários exemplos de fractais, tais como séries temporais nas áreas de climatologia e hidrologia, séries de imagens de ressonância magnética funcional do cérebro humano, movimento turbulento de fluidos e séries de dados financeiros, dentre outros, que envolvem uma extensa faixa de escalas de medição \cite{beran94} \cite{percival00}, ou seja, que não ocorrem numa escala temporal ou espacial característica.

A Fig. \ref{fig:drexel} ilustra que o tráfego Ethernet coletado numa rede local da Universidade de Drexel é auto-similar e altamente impulsivo em quatro escalas temporais de agregação ($10$ ms, $100$ ms, $1$ s e $10$ s). A série na escala de $100$ ms foi obtida por meio de uma agregação da série na escala de $10$ ms, ou seja, uma ordenada da série de $100$ ms corresponde à soma dos \emph{bytes} em 10 \emph{bins} consecutivos da série na escala de $10$ ms. O mesmo procedimento foi  aplicado para obtenção das séries nas escalas de $1$ s e $10$ s, que são agregações das séries nas escalas de $100$ ms e $1$ s, respectivamente. É surpreendente observar que agregações sucessivas não suavizam o tráfego (a suavização aconteceria se o tráfego fosse bem modelado pelo processo de Poisson\footnote{Agregações sucessivas de uma série temporal de Poisson tendem a produzir séries do tipo RB, conforme ilustrado pela Fig. \ref{fig:bellcore} (vide séries de Poisson da coluna central).} \cite{paxson95}). Note-se que a alternância de períodos de surtos e de suavidade é preservada nas quatro escalas de tempo em questão. Isto permite afirmar que não há uma escala de tempo característica para a ocorrência de surtos. 
Esta propriedade de invariância com relação à mudança de escala temporal é conhecida na literatura como ``\emph{scaling invariance}'' ou ``\emph{scaling behavior}'' (que devem ser entendidos como sinônimos de auto-similaridade) \cite{feldmann98}, \cite{park05}.

\begin{figure}
	\begin{center}
		\includegraphics{figuras/Drexel.eps}
	\end{center}
	\caption{Visualização do tráfego \emph{Fast} Ethernet (100 Mbps) coletado num servidor WWW/Email/FTP da Universidade de Drexel \cite{cappe02} em quatro diferentes níveis de agregação: $10$ ms, $100$ ms, $1$ s e $10$ s (de cima para baixo). Este \emph{trace} tem um tamanho total de $3$ h.}
	\label{fig:drexel}
\end{figure}

Na Fig. \ref{fig:bellcore}, pode-se observar, qualitativamente, as diferenças entre o tráfego Ethernet real e o tráfego simulado por meio do modelo de Poisson \cite{leland94}. Para o tráfego Ethernet (à esquerda), note-se que a alternância de períodos de surtos e de suavidade também é preservada em várias escalas de tempo, como na Fig. \ref{fig:drexel}. Para o tráfego Poisson (ao centro), note-se que períodos de surto de tráfego ocorrem em escalas de tempo menores (10 milissegundos e 0,1 segundo) e que a agregação resulta numa série de nível aproximadamente constante (vide o gráfico em que a unidade de tempo é igual a 10 segundos), o que é uma clara indicação de que as propriedades estatísticas do tráfego Poisson não são mantidas ao longo de várias escalas de agregação. Para o tráfego sintetizado por meio de um modelo auto-similar (à direita), pode-se observar que também há alternância de períodos de surtos e de suavidade em várias escalas de tempo.
\begin{figure}
	\begin{center}
		\includegraphics{figuras/bellcore.eps}
	\end{center}
	\caption{Comparação de tráfego Ethernet real e sintetizado \cite{leland94}. À esquerda tem-se o tráfego real, ao centro tráfego simulado utilizando-se o modelo de Poisson e à direita tráfego gerado por meio de um modelo auto-similar.}
	\label{fig:bellcore}
\end{figure}

%Outros trabalhos \cite{riedi97} \cite{feldmann98} \cite{gilbert99} \cite{ribeiro99} mostraram que o tráfego \ac{WAN} é \emph{multifractal} (com distribuição marginal não-Gaussiana) em escalas refinadas de tempo (até centenas de milissegundos), em contraste com o comportamento \emph{monofractal} (ou auto-similar) que foi observado para o tráfego \ac{LAN} \cite{leland94}.

\section{O Expoente de Hurst}
% Discutir estatística R/S
Por razões históricas, o grau de persistência (LRD) de uma série temporal é caracterizado pelo parâmetro $H$, $0<H<1$, de Hurst\footnote{Hurst, um hidrologista, observou que a série histórica do nível mínimo anual do rio Nilo possui memória longa. Esta série tem sido registrada há centenas de anos.} \cite{hurst51}. Uma série temporal é  LRD (auto-similar) quando $1/2<H<1$ e tem memória curta ou dependência de curta duração (\ac{SRD}) para $0<H\leq1/2$ \cite{beran94}. Quanto mais próximo $H$ estiver de $1$, maior será o grau de persistência da série. Informalmente, diz-se que uma série é monofractal quando $H$ é invariante no tempo e multifractal	quando $H$ varia no tempo de forma determinística ou aleatória. %(o conceito de série multifractal será formulado na seção \ref{sec:modelos}). 
Os artigos \cite{feldmann98}, \cite{riedi97}, \cite{gilbert99}, \cite{ribeiro99} mostraram que o tráfego \ac{WAN} pode ser multifractal (com distribuição marginal não-Gaussiana) em escalas refinadas de tempo (até centenas de milissegundos), em contraste com o comportamento monofractal (ou auto-similar) que foi observado para o tráfego \ac{LAN} \cite{leland94}. A Fig. \ref{fig:mwm} mostra uma realização produzida pelo modelo \ac{MWM} de Riedi \emph{et al} \cite{riedi99}.
%\footnote{É interessante observar que algumas séries de volatidade de log-retornos de preços de ações são visualmente parecidas com séries multifractais de tráfego geradas pelo modelo MWM (vide Fig. \ref{fig:mwm}), apresentando, inclusive, a característica de memória longa (vide os \emph{plots} para os retornos do IBOVESPA, Petrobrás e Banespa nas páginas 160, 163 e 165, respectivamente, da referência \cite{morettin03}).}.
%Supõe-se que as séries de retornos sejam geradas por processos integrados de ordem $d$ $\sim I(d)$.
%Um processo $X_t$ não-estacionário é integrado de ordem $d$ quando a série estacionária $W_t = \Delta^d X_t = (1-B)^d X_t$, onde $BX_t = X_{t-1}$. Por exemplo, um processo ARIMA($p=1$,$d=1$,$q=0$) é um processo $I(1)$. Já um ARFIMA$(1,\frac{1}{2};0)$ é do tipo $I(\frac{1}{2})$ (integração fracionária).
\begin{figure}
	\centering
		\includegraphics* [width=9.5cm, height=7.5cm] {figuras/MWM.eps}
	\caption{Tráfego multifractal simulado por meio do modelo MWM. Note-se que a impulsividade da série varia no tempo (heterocedasticidade).}
	\label{fig:mwm}
\end{figure} 

Vários pesquisadores notaram que a \ac{FAC} de várias séries temporais empíricas em hidrologia \cite{hurst51}, economia \cite{granger66}, \cite{mandelbrot71} e telecomunicações \cite{leland94}, \cite{paxson95} apresenta um decaimento extremamente lento, do tipo hiperbólico, para grandes valores de \emph{lag}. Isto quer dizer que a dependência entre observações distantes daquelas séries, apesar de serem pequenas, não são de maneira alguma desprezíveis. Em contraste, séries SRD têm FACs com decaimento rápido, do tipo exponencial. 
A Fig. \ref{fig:acf-lrd1h09noIIR} mostra a FAC de uma série LRD com $H=0,9$ e $N=4096$ amostras, que foi simulada pelo gerador \emph{wavelet} de tráfego desenvolvido por Mello, Lima, Lipas e Amazonas \cite{mello07}, \cite{lima07b}. Observe que a FAC é altamente persistente, apresenta um decaimento lento para zero e tem valores significativos até o \emph{lag} 200. Tráfego agregado também pode apresentar uma FAC que mistura as características de memória longa e de dependência de curta duração (\ac{SRD}) \cite{paxson97}, \cite{duffield97}, \cite{ma01}, comportamento típico da classe de modelos \ac{FARIMA} (ou \ac{ARFIMA}) \cite{granger80}, \cite{hosking81}, que será abordada no Capítulo \ref{cap:modelagem}. 

\begin{figure}
	\begin{center}
		\includegraphics[height=8cm,keepaspectratio]{figuras/acf-lrd1h09noIIR.eps}
	\end{center}
	\caption{FAC de uma série LRD com $H=0,9$ e $N=4096$ amostras. A linha contínua mostra a função de autocorrelação do melhor modelo auto-regressivo (AR) que pôde ser ajustado pela função \texttt{ar} do \emph{software} \texttt{S+FinMetrics} \cite{insightful:url} à série segundo o critério \ac{AIC} \cite{akaike73}. No caso, obteve-se um modelo AR$(15)$. Note que o decaimento da autocorrelação do modelo AR$(15)$ ajustado decai mais rapidamente (exponencialmente) para zero do que o da série LRD.}
	\label{fig:acf-lrd1h09noIIR}
\end{figure}

A memória longa é caracterizada no domínio da freqüência pela existência de uma singularidade do tipo $1/f^{\alpha},\,0<\alpha<1$ ($\alpha = 2H-1$), para $f\rightarrow0$. 
A Fig. \ref{fig:psd-lrd-srd} mostra que a \ac{DEP} de um modelo LRD da classe \ac{FD}\footnote{O modelo FD$(d)$ corresponde ao processo FARIMA$(p,d,q)$ com $p=q=0$, em que $p$ denota o parâmetro autoregressivo e $q$ o de média móvel.} \cite{granger80},  \cite{hosking81} com parâmetro $d=0,4$ ($d=H-1/2$) apresenta o comportamento  $1/f^{\alpha}$ na origem do espectro, ao passo que um modelo \ac{AR} de ordem $4$ (AR$(4)$) não. A literatura costuma referir-se aos  processos LRD como ``ruídos $1/f$''.  
\begin{figure}
	\begin{center}
	\centering
		\includegraphics[height=8cm,keepaspectratio]{figuras/psd-lrd-srd.eps}
		\caption{DEPs para modelos AR(4) e FD(0,4) de mesma potência.}
	\label{fig:psd-lrd-srd}
	\end{center}
\end{figure}

\section{LRD e Auto-Similaridade}
\label{fractal:sec:def-lrd-ss}
%Os conceitos de LRD e auto-similaridade estão formulados no artigo que compõe o apêndice \ref{apendice-B} deste relatório. 

\subsection{Dependência de Longa Duração}
\label{fractal:subsec:lrd}

Considere um processo aleatório estacionário $\rvx_t,\, t\in \mathbb{Z}$, com média $\mu_{\rvx}$ e variância $\sigma^2_{\rvx}$. Sejam $x_1,x_2,\ldots,x_N$ observações de uma realização $x_t$. Se as variáveis aleatórias $\rvx_1,\rvx_2,\ldots,\rvx_N$ forem independentes ou não-correlacionadas, então, a variância de $\bar{x}$ (média amostral) será dada por
\begin{equation}
  \label{eq:var-media-amostral}
   \sigma^2_{\bar{x}} = \frac{\sigma^2_{\rvx}}{N}.
\end{equation}
Se a amostra for suficientemente grande, a distribuição amostral do estimador $\bar{x}$ será normal. A expressão do intervalo de confiança para $\mu_{\rvx}$, ao nível de confiança $(1-\beta)$, é dada por\footnote{Dada uma probabilidade $1-\beta$, encontra-se um valor $z_{\beta/2}$ tal que $P\{-z_{\beta/2}<Z<z_{\beta/2}\}=1-\beta$ ($z_{\beta/2}=1,96$ para $1-\beta=95\%$).} 
\begin{equation}
\label{eq:int-conf-media-amostral}
 \bar{x} - z_{\beta/2}\frac{\sigma_{\rvx}}{\sqrt N}  \leq \mu_{\rvx} \leq \bar{x} 
           + z_{\beta/2}\frac{\sigma_{\rvx}}{\sqrt N},
\end{equation}
em que $z_{\beta/2}$ denota o quantil\footnote{O quantil $q_\alpha$ de uma função de distribuição $F_{\rvx}$ é o valor para o qual se tem $F_{q_\alpha}=\alpha$ \cite[pág.181]{jain91}. A mediana, por exemplo, corresponde ao quantil $q_{0,5}$.} 
$q_{(1-\beta/2)}$ da distribuição normal padrão \cite{beran94}.


\begin{defi}[Dependência de Longa Duração]
\label{def:lrd}
$\rvx_t$ é um processo com dependência de longa duração ou memória longa se existirem constantes $\alpha$ e $C_P$, satisfazendo $0<\alpha<1$ e $C_P>0$, tais que \cite{beran94}, \cite{percival00}, \cite{taqqu03}
\begin{equation}
  \label{eq:lrd}
  \lim_{f\rightarrow0}\frac{P_{\rvx}(f)}{C_P|f|^{-\alpha}} = 1 \, ,
\end{equation}	
em que $P_{\rvx}(f)$ denota a DEP de $\rvx_t$ e  $f$ representa a freqüência normalizada ($ -1/2 \leq f \leq 1/2$), em $\text{ciclos/amostra}$. 
\end{defi}
Portanto, a DEP de processos LRD tende a infinito na freqüência zero (singularidade do tipo $1/f^\alpha$ na origem do espectro). Note-se que a definição \ref{def:lrd} é assintótica, pois o formato da DEP em freqüências afastadas da origem não é especificado. É mais comum caracterizar-se a memória longa pelo parâmetro $H$ de Hurst:
\begin{equation}
	\label{rel-H-alpha}
	H = \frac{\alpha+1}{2},\,\,\quad 1/2<H<1 \, .
\end{equation}
Quanto maior o valor de $H$, maior é o grau de memória longa do processo. 

Uma definição alternativa pode ser dada no domínio do tempo, em termos da autocorrelação $R_{\rvx}(\tau)$. $\rvx_t$ é um processo do tipo $1/f^{\alpha}$ se a sua autocorrelação $R_{\rvx}(\tau)$, para valores suficientemente grandes do \emph{lag} $\tau$, decresce segundo uma função potência (isto é, o decaimento para zero é extremamente lento e do tipo hiperbólico):
\begin{equation}
  \label{def:lrd2}
	\lim_{\tau\rightarrow\infty}\frac{R_{\rvx}(\tau)}{C_R \tau^{-(1-\alpha)}} = 1 \, ,
\end{equation}	
em que $C_R>0$. 

A singularidade na origem do espectro implica a validade da relação
\begin{equation}
  \label{eq:not-sum}
	\sum_{\tau=-\infty}^{\infty}R_{\rvx}(\tau) = \infty\, ,
\end{equation}	
para $1/2<H<1$, ou seja, as autocorrelações decaem para zero tão lentamente que não são somáveis. A Eq. (\ref{eq:not-sum}) afirma que a dependência estatística entre eventos distantes diminui muito lentamente com o aumento do \emph{lag} $\tau$. Deste modo, a razão entre autocorrelações para valores suficientemente grandes do \emph{lag} não se altera de modo apreciável. Este comportamento é radicalmente diferente do apresentado por um processo da classe \ac{ARMA} \cite{box76},\cite{tsay05}, em que a autocorrelação decresce rapidamente (de forma exponencial) para zero:
\begin{equation}
  \label{eq:fast-decay-autocorr}
	|R_{\rvx}(\tau)| \leq Cr^{-\tau} ,\quad \tau=1,2,\ldots\,,
\end{equation}
em que $C>0$ e $0<r<1$. Processos ARMA possuem dependência de curta duração.

Se $\rvx_t$ for LRD, a variância de $\bar{x}$ decresce com o tamanho $N$ da amostra mais lentamente do que no caso tradicional (variáveis independentes ou não-correlacionadas) e da seguinte maneira \cite[pág.6]{beran94}
\begin{equation}
  \label{eq2:var-media-amostral}
   \sigma^2_{\bar{x}} \approx \sigma^2_{\rvx} c(\rho_{\rvx})N^{\alpha -1},
\end{equation}
em que $c(\rho_{\rvx})$ é definido por
\begin{equation}
   \underset{N \rightarrow \infty} \lim\, N^{-(1+\alpha)} \underset{i \neq j} \sum \rho_{\rvx}(i,j).
\end{equation}
Neste caso, a distribuição de $\bar{x}$ é assintoticamente Gaussiana, com $E[\bar{x}] = \mu_{\rvx}$. Por outro lado, a variância da média amostral de séries ARMA e de Markov tem o seguinte comportamento assintótico \cite[pág.5]{beran94}:
\begin{equation}
  \label{eq:var-media-amostral-ARMA}
   \sigma^2_{\bar{x}} \approx \sigma^2_{\rvx} c(\rho_{\rvx})N^{-1}.
\end{equation}


O comportamento LRD de $x_t$ faz com que a estimação de parâmetros como $\bar{x}$ seja mais difícil do que quando as observações são não-correlacionadas. Neste caso, a equação do intervalo de confiança para $\mu_{\rvx}$ (dada por (\ref{eq:int-conf-media-amostral})) não é aplicável. De fato, para um determinado nível de confiança $(1-\beta)$ , o intervalo de confiança dado deve ser ``esticado'' por meio da multiplicação do mesmo por um fator $F$ dado por \cite[pág.9]{beran94}  
\begin{equation}
   F = N^{\alpha/2} \sqrt{c(\rho_{\rvx})}.
\end{equation}
Observe-se que este fator de correção $F$ cresce com $N$, sendo que no limite diverge para o infinito.

Pode-se adquirir uma certa intuição das propriedades de um processo LRD por meio da análise dos seus processos agregados.
O processo agregado de $\rvx_t$ de grau $M$, denotado por $X^{(M)}_t$, corresponde a uma média móvel de blocos não-sobrepostos de $\rvx_t$ de tamanho $M$, ou seja,
\begin{equation}
	\label{eq:proc-agreg}
	X^{(M)}_i = \frac{1}{M} \sum_{t=M(i-1)+1}^{Mi} \rvx_t.
\end{equation}
A seguinte propriedade é válida para um processo $\rvx_t$ com memória longa  \cite{athina04}:
\begin{equation}
  \label{eq:var-agg}
	\lim_{M\rightarrow\infty}\frac{\text{Var}\,X^{(M)}_t}{M^{(2H-2)}} = c,
\end{equation}
em que $c$ é uma constante. Observe-se que a agregação equivale a uma operação de mudança de escala temporal.

Conforme visto anteriormente, a Fig. \ref{fig:bellcore} ilustra que agregações sucessivas de um processo aleatório SRD (como o de Poisson) tende a suavizar o processo agregado resultante depois de algumas mudanças de escala. Por outro lado, (\ref{eq:var-agg}) afirma que o grau de suavidade da seqüência de processos agregados $\{X^{(M)}_t\}_{M=2,3,\ldots}$, associados a um processo LRD $\rvx_t$, aumenta com o aumento de $M$ muito mais lentamente do que no caso de processos SRD, em que a variância do processo agregado decai proporcionalmente a $M^{-1}$ para $M\rightarrow\infty$. Sendo assim, depreende-se que o processo agregado é estatisticamente similar  ao processo original, no sentido de que um número finito de agregações sucessivas não destrói o caráter impulsivo do processo original. Portanto, (\ref{eq:var-agg}) sugere que as propriedades de dependência de longa duração e auto-similaridade (que será definida a seguir) estejam intimamente relacionadas.    
 
\subsection{Auto-Similaridade}
\label{fractal:subsec:ss}

\begin{defi}[Processo $H\text{-ss}$]
\label{def:auto-simi}
Um processo estocástico $\{\rvy_t\}_{t\in \mathbb{R}}$ é auto-similar com parâmetro $0<H<1$, ou seja, é \emph{H}-ss (\emph{self-similar with parameter $H$}) se, para qualquer $a>0$,
\begin{equation}
  \{\rvy(t)\} \stackrel{d}= \{a^{-H} \rvy(at)\},  
	\label{eq:int-ordem1}
\end{equation}
em que $\stackrel{d}=$ denota igualdade entre as distribuições finito-dimensionais \cite{taqqu03}.
\end{defi}

Um processo $H\text{-ss}$ é LRD se $1/2<H<1$. O processo Movimento Browniano (de tempo contínuo) \cite{mandelbrot68}, também conhecido como processo de Wiener, satisfaz a definição \ref{def:auto-simi}, sendo auto-similar com $H=1/2$ (mas não é LRD). Se o processo $\rvx_t = \Delta \rvy_t$, denominado processo de incrementos de $\rvy_t$ ou primeira diferença de $\rvy_t$, for estacionário, então $\rvy_t$ é denominado $H\text{-sssi}$ (\emph{H self-similar with stationary increments} - auto-similar com incrementos estacionários). Neste caso, o processo $H\text{-sssi}$ $\rvy_t$ é um processo integrado de ordem 1, $\rvy_t\sim I(1)$.

Se os momentos de $\rvy(t)$ de ordem menor ou igual a $q$ existirem, pode-se concluir a partir de (\ref{eq:int-ordem1}) que \cite{athina04}
\begin{equation}
	\label{eq:momentos-int-ordem1}
	E|\rvy_t|^q = E|\rvy_1|^q |t|^{qH}.
\end{equation}  
Portanto, o processo $\rvy_t \sim I(1)$ não pode ser estacionário. 

Assumindo-se $E[\rvy_t]=0$ com o intuito de simplificar a notação\footnote{O que implica $E[\rvx_t]= E[\rvy_t - \rvy_{t-1}]=0$}, demonstra-se que a autocovariância de $\rvy_t$ é dada por \cite{beran94}
\begin{equation}
	\label{eq:cov-int-ordem1}
	C_{\rvy}(t,s) = E[\rvy_t \rvy_s] = \frac{\sigma^2_\rvx}{2}[t^{2H} + s^{2H} - (t-s)^{2H}].
\end{equation}  
em que $\sigma^2_\rvx = E[(\rvy_t-\rvy_{t-1})^2]=E[\rvy^2(1)]$ é a variância do processo de incrementos $\rvx_t$. 

Considere a versão amostrada $\rvy_t$, $t\in \mathbb{Z}$, de um processo $\rvy_t$ $H\text{-sssi}$, em que o período de amostragem é unitário. Existem vários processos não-Gaussianos $\rvy_t$ $H\text{-sssi}$. Entretanto, para cada valor de $H \in (0,1)$ há exatamente um único processo Gaussiano $\rvy_t$ $H\text{-sssi}$, denominado movimento Browniano fracionário de tempo discreto (\ac{DFBM})\cite{beran94}, \cite{percival00}. O ruído Gaussiano fracionário (\ac{FGN}), proposto por Mandelbrot e van Ness em 1968 \cite{mandelbrot68}, corresponde ao processo de incrementos do DFBM. O FGN é um modelo bastante utilizado em simulações de tráfego LRD \cite{mello07}, \cite{paxson97}, \cite{backar00} \footnote{Os modelos DFBM e FGN são não-paramétricos e não são utilizados para se fazer previsão de valores futuros de tráfego. 
%Tais processos são introduzidos pelo apêndice \ref{apend-A}. Esse mesmo apêndice também apresenta o modelo não-paramétrico MWM. 
O processo FGN é definido no domínio da freqüência a partir de uma prescrição específica de DEP \cite{paxson97}, ao passo que o MWM utiliza a análise de multirresolução de Haar e é baseado numa cascata binomial multiplicativa no domínio \emph{wavelet} (vide seção \ref{model:sec:non-param}) \cite{riedi99}.}. 

\begin{defi}[Auto-Similaridade Exata de Segunda Ordem]
\label{def:as-exata-2a-ordem}
Seja o processo estacionário de tempo discreto $\rvx_t = \rvy_t-\rvy_{t-1}$. $\rvx_t$ é um processo exatamente auto-similar de segunda ordem com parâmetro de Hurst $H$ $(1/2<H<1)$ se a sua autocovariância existe e for dada por \cite{beran94}    
\begin{equation}
  \label{eq:cov-as}
  C_{\rvx}(\tau) = \frac{\sigma^2_\rvx}{2}[ |\tau+1|^{2H} - 2|\tau|^{2H} + |\tau-1|^{2H}], \quad \tau=\ldots,-1,0,1,\ldots \quad.
\end{equation}
\end{defi}

Pode-se demonstrar que a autocovariância dada por (\ref{eq:cov-as}) satisfaz \cite{beran94}
\begin{equation}
  \label{eq:cov-as-lrd}
  \lim_{\tau\rightarrow\infty} \frac{C_{\rvx}(\tau)}{\sigma^2_{\rvx} \tau^{2H-2}H(2H-1)} = 1, 
\end{equation}
ou seja, $C_{\rvx}(\tau)$ tem um decaimento hiperbólico. Portanto, auto-similaridade de segunda ordem implica LRD quando $1/2<H<1$. 

Considere o processo agregado $X^{(M)}_t$ de um processo exatamente auto-similar de segunda ordem $\rvx_t$, ao nível de agregação $M$. Demonstra-se que 
\begin{equation}
  \label{eq:cov-agg-as}
  C_\rvx^{(M)}(\tau) = C_{\rvx}(\tau),\quad\quad M=2,3,\ldots 
\end{equation}

A Eq. (\ref{eq:cov-agg-as}) diz que as estatísticas de segunda ordem do processo original não mudam com a mudança de escala, o que justifica o termo ``exatamente auto-similar de segunda ordem''. 

\begin{defi}[Auto-Similaridade Assintótica de Segunda Ordem]
\label{def:as-assint-2a-ordem}
Um processo $\rvx_t$ é  assintoticamente auto-similar de segunda ordem com parâmetro de Hurst $H$ $(1/2<H<1)$ se a sua autocovariância e a autocovariância do seu processo agregado estão relacionadas por \cite{athina04}    
\begin{equation}
  \label{eq:cov-assint-as}
  \lim_{M\rightarrow\infty} C_\rvx^{(M)}(\tau) = C_{\rvx}(\tau).
\end{equation}
\end{defi}

Tsybarov e Georganas \cite{tsybakov97} demonstram que (\ref{eq:var-agg}) implica (\ref{eq:cov-assint-as}). Portanto, um processo LRD também é assintoticamente auto-similar de segunda ordem.

\section{Impulsividade}
\label{fractal:sec:imp}

Vários fenômenos exibem comportamento impulsivo, tais como ruído atmosférico de baixa freqüência, ruído produzido pelo homem, ruído acústico submarino, transitórios em linhas de transmissão, atividade sísmica \cite{mandelbrot68}, \cite{pillai87}, \cite{bouvet89}, \cite{middleton77}, \cite{pierce97}, séries financeiras \cite{mandelbrot63}, \emph{clusters} de erros em circuitos telefônicos \cite{berger63} e o tráfego em redes de computadores \cite{paxson95}. Neste contexto, a distribuição de probabilidades do tipo \emph{stable} (estável) \cite{levy25} é uma ferramenta básica de modelagem estatística de sinais impulsivos \cite{shao93}. O uso das distribuições estáveis é justificado pelo teorema central do limite generalizado \cite{nolan:url}, o qual afirma que, se o limite da soma de variáveis aleatórias independentes e identicamente distribuídas (i.i.d.) converge, então este limite só pode ser uma variável aleatória com distribuição estável.  

%O apêndice \ref{apend-B} introduz os conceitos de variável aleatória com distribuição de cauda pesada, distribuições estáveis, impulsividade e codiferença generalizada (para medição da dependência dos processos que possuem variância infinita) \cite{athina04}.

\subsection{Distribuições Estáveis}

\begin{defi}[Distribuição de Cauda Pesada]
\label{def:ht}
Uma variável aleatória $\rvx$ possui uma distribuição de cauda pesada com índice $\alpha$ se \cite{taqqu94} 
\begin{equation} 
	\label{eq:ht-dist}
	P(\rvx \geq x) \sim cx^{-\alpha}L(x), \;\;\;\;\;\;\;x\rightarrow\infty\,, 
\end{equation}
para $c>0$ e $0<\alpha<2$, em que $L(x)$ é uma função positiva que varia lentamente para grandes valores de $x$, ou seja, $\lim_{x\rightarrow\infty}L(bx)/L(x)=1$ para qualquer $b$ positivo.
\end{defi}
A Eq. (\ref{eq:ht-dist}) afirma que observações de uma variável aleatória com distribuição de cauda pesada podem
ocorrer, com probabilidades não-desprezíveis, com valores bastante distantes da média (\emph{outliers}). 
Portanto, esse tipo de variável aleatória apresenta alta variabilidade. 

Um exemplo simples de distribuição de cauda pesada é a distribuição de Pareto I \cite{gubner06},\cite{wiki-Pareto:url}, que é definida em termos de sua função distribuição complementar de probabilidade (função de sobrevivência):   
\begin{equation}
	\label{eq:Pareto-I}
\bar{F}(x) = P(\rvx \geq x) =
	\begin{cases}
		\left(\frac{x}{x_m}\right)^{-\alpha}, & \,\,x\geq x_m\,,  \\
		\,\,1, & \,\, x < x_m\,, 
	\end{cases}
\end{equation}
As Figs. \ref{fig:pdf-Pareto-I} e \ref{fig:cdf-Pareto-I} ilustram as funções densidade e distribuição de probabilidade Pareto I. 

\begin{figure}
	\begin{center}
		%\includegraphics[height=8cm,keepaspectratio]{figuras/acf-lrd1h09noIIR.eps}
		\includegraphics[height=8cm,keepaspectratio]{figuras/pdf-Pareto-I.eps}
	\end{center}
	\caption{Funções densidade de probabilidade de Pareto I para vários valores de $k=\alpha$ com $x_m =1$. Observe que a função decai lentamente para zero para valores grandes de $x$. A função tende para o impulso $\delta(x-x_m)$ quando $k\rightarrow\infty$.}
	\label{fig:pdf-Pareto-I}
\end{figure}

\begin{figure}
	\begin{center}
		%\includegraphics[height=8cm,keepaspectratio]{figuras/acf-lrd1h09noIIR.eps}
		\includegraphics[height=8cm,keepaspectratio]{figuras/cdf-Pareto-I.eps}
	\end{center}
	\caption{Funções distribuição de probabilidade de Pareto I para vários valores de $k=\alpha$ com $x_m =1$.}
	\label{fig:cdf-Pareto-I}
\end{figure}

As estatísticas de ordem $p$ das distribuições de cauda pesada são finitas se, e somente se, $p \leq \alpha$. É por essa razão que essas distribuições têm variância infinita (a média é infinita se $\alpha<1$). As distribuições de cauda pesada são também conhecidas como distribuições de probabilidade com variância infinita. 

%An $\alpha$-stable (non-Gaussian) random process\cite{taqqu94} $X_t$ can be generated if its wavelet coefficients also form an $\alpha$-stable process\cite{popescu97}. 

Um membro importante da classe das distribuições de cauda pesada é a \textbf{distribuição estável}, descoberta por Lévy na década de 1920 \cite{levy25}. A distribuição estável não possui expressão analítica\footnote{Com exceção dos casos-limite $\alpha=1$ (Cauchy) e $\alpha=2$ (Gaussiana).}, mas pode ser definida em termos da sua função característica\footnote{Definida como a transformada de Fourier da função densidade de probabilidade\cite{stark02}.}. Uma variável aleatória $\rvx$ estável é definida pela seguinte função característica \cite{taqqu94}: 
\begin{equation} 
	\label{eq:alpha-stable}
	\Phi_{\rvx}(w) = E[e^{jw \rvx}] = \int_{-\infty}^{\infty} f_{\rvx}(x)e^{jwx}\, dx =
	\text{exp} \{ j\mu w - |\sigma w|^{\alpha} [1-j \eta \,\text{sign}(w) \varphi(w,\alpha)] \},
\end{equation}
em que,
\begin{equation} 
\varphi(w,\alpha)=
	\begin{cases}
		\tan{(\alpha \pi/2)} & \text{se $\alpha\neq1$} \\
		-\frac{2}{\pi}\ln{|w|} & \text{se $\alpha=1$}, 
	\end{cases}
\end{equation}
e $\text{sign}(.)$ denota a função \texttt{sinal}, $\alpha$  ($0<\alpha \leq 2$) é o \textbf{expoente característico}, $\mu$ ($\mu \in \mathbb{R}$) é o \textbf{parâmetro de localização}, $\eta$ ($-1 \leq \eta \leq 1$) é o \textbf{parâmetro de assimetria} e $\sigma \geq 0$ é o \textbf{parâmetro de dispersão} ou \textbf{escala}.  A notação $S_{\alpha}(\sigma,\eta,\mu)$ denota a distribuição definida por (\ref{eq:alpha-stable}). A Fig. \ref{fig:sinal-alpha-stable} ilustra uma realização $\alpha$-\emph{stable}, simulada por meio da função \texttt{stablernd}, da biblioteca de funções \texttt{STABLE4.0} \cite{robust:url} para \texttt{MATLAB}. 
As Figs. \ref{fig:Levy-pdfs}, \ref{fig:Levyskew-pdfs}, \ref{fig:Levy-cdfs} e \ref{fig:Levyskew-cdfs} ilustram algumas funções densidade e distribuição de probabilidade de variáveis aleatórias estáveis. 
   
\begin{figure}
	\begin{center}
	\centering
		\includegraphics[height=8cm,keepaspectratio]{figuras/sinal-alpha-stable.eps}
		\caption{Simulação de uma realização com distribuição $S_{1,2}(1,1,0)$ ($256$ amostras).}
	\label{fig:sinal-alpha-stable}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
	\centering
		\includegraphics[height=8cm,keepaspectratio]{figuras/Levy-pdfs.eps}
		\caption{Funções densidade de probabilidade estáveis simétricas centradas em zero \cite{wiki-stable:url}. Os parâmetros $\beta$ e $c$ correspondem aos parâmetros $\eta$ e $\sigma$, respectivamente, de (\ref{eq:alpha-stable}).}
	\label{fig:Levy-pdfs}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
	\centering
		\includegraphics[height=8cm,keepaspectratio]{figuras/Levyskew-pdfs.eps}
		\caption{Funções densidade de probabilidade estáveis assimétricas \cite{wiki-stable:url}. }
	\label{fig:Levyskew-pdfs}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
	\centering
		\includegraphics[height=8cm,keepaspectratio]{figuras/Levy-cdfs.eps}
		\caption{Funções distribuição de probabilidade simétricas \cite{wiki-stable:url}. }
	\label{fig:Levy-cdfs}
	\end{center}
\end{figure}

\begin{figure}
	\begin{center}
	\centering
		\includegraphics[height=8cm,keepaspectratio]{figuras/Levyskew-cdfs.eps}
		\caption{Funções distribuição de probabilidade assimétricas \cite{wiki-stable:url}. }
	\label{fig:Levyskew-cdfs}
	\end{center}
\end{figure}

O parâmetro $\alpha$ especifica o grau de impulsividade da variável aleatória (o peso da cauda da distribuição aumenta conforme $\alpha$ diminui). Para qualquer constante real $a$, $\rvx+a$ tem distribuição $S_{\alpha}(\sigma,\eta,\mu+a)$. Para $a>0$ e $\alpha\neq1$, $a\rvx$ tem distribuição $S_{\alpha}(a\sigma,\eta,a\mu)$. A distribuição é simétrica em relação a $\mu$ se $\eta = 0$. A distribuição Gaussiana é um caso particular da estável para $\alpha=2$, pois, neste caso,  tem-se que $\Phi_{\rvx}(w) = \text{exp}(-\sigma^2 w^2 + j\mu w)$ corresponde à função característica de uma variável aleatória Gaussiana com média $\mu$ e variância $2 \sigma^2$. Quando $\eta=\mu=0$ diz-se que $\rvx$ é do tipo estável simétrica (\emph{symmetric} \emph{stable} (S$\alpha$S)) com $\Phi_{\rvx}(w) = \text{exp}(-\sigma^{\alpha} |w|^{\alpha})$.

Demonstra-se que, se $\rvx \sim S_{\alpha}(\sigma,\eta,\mu)$ com $0<\alpha<2$, então,
\begin{equation}
\label{p-law-stable}
	\begin{cases}
		\lim_{\lambda \rightarrow\infty} \frac{P(\rvx>\lambda)}{\lambda^{-\alpha}} = C_{\alpha} \frac{1+\eta}{2} \sigma^{\alpha}, \\
		\lim_{\lambda \rightarrow\infty} \frac{P(\rvx<-\lambda)}{\lambda^{-\alpha}} = C_{\alpha} \frac{1-\eta}{2} \sigma^{\alpha}, 
	\end{cases}
\end{equation}
em que
\begin{equation}
	C_{\alpha} = \left(\int_{0}^{\infty} x^{-\alpha} \sin x \right)^{-1} =
	\begin{cases}
		\frac{1 - \alpha}{\Gamma(2-\alpha)\cos(\pi \alpha/2)} & \text{se}\,\, \alpha \neq 1 \\
		2/\pi & \text{se}\,\,\alpha=1.
	\end{cases}
\end{equation}
Portanto, (\ref{p-law-stable}) mostra que a função de sobrevivência de $\rvx$ decresce segundo uma função potência para grandes valores de $\lambda$.  

Distribuições estáveis possuem duas propriedades importantes: estabilidade e o teorema central do limite generalizado \cite{shao93}, \cite{nolan:url}, que serão enunciadas a seguir.

\begin{teo}[Propriedade de Estabilidade]
\label{stable}
Uma variável aleatória $\rvx$ é estável se e somente se para quaisquer variáveis aleatórias independentes $\rvx_1$ e $\rvx_2$ que tenham a mesma distribuição de $\rvx$, e para constantes arbitrárias $a_1$, $a_2$, existem constantes $a$ e $b$ tais que 
\begin{equation}
	a_1\rvx_1 + a_2\rvx_2 \overset{d}{=} a\rvx + b,
\end{equation}
em que $\overset{d}{=}$ denota igualdade de distribuição de probabilidades \cite{taqqu94}. 
\end{teo}
Utilizando-se a função característica da distribuição estável, pode-se demonstrar uma propriedade mais geral: se $\rvx_1,\rvx_2,\ldots,\rvx_N$ são independentes e seguem distribuições estáveis de mesmo $(\alpha,\eta)$, então todas as combinações lineares da forma $\sum_{j=1}^{N}a_j \rvx_j$ são estáveis com os mesmos parâmetros $\alpha$ e $\eta$.  

O teorema central do limite diz que a soma normalizada de variáveis aleatórias independentes e identicamente distribuídas (i.i.d.) com variância $\sigma^2$ e média $\mu$ finitas converge para uma distribuição Gaussiana. Formalmente, o teorema afirma que 
\begin{equation}
	\label{TLC}
	\frac{\bar{\rvx} - \mu}{\sigma/\sqrt{N}} \overset{d}{\rightarrow} \rvx \sim N(0,1) \quad \text{para} \quad N\rightarrow\infty.
\end{equation}
A relação (\ref{TLC}) pode ser reescrita como 
\begin{equation}
	\label{TLC2}
	a_N(\rvx_1+\rvx_2+\ldots+\rvx_N) - b_N \overset{d}{\rightarrow} \rvx \sim N(0,1) \quad \text{para} \quad \rvx\rightarrow\infty,
\end{equation}
em que $a_N = 1/(\sigma\sqrt{N})$ e $b_N=\sqrt{N}\mu/\sigma$.

\begin{teo}[Teorema Central do Limite Generalizado]
\label{TLCg}
Seja $\{\rvx_1,\rvx_2,\rvx_3\ldots\}$ uma seqüência de variáveis aleatórias i.i.d. Há constantes $a_N>0$, $b_N \in \mathbb{R}$
e uma variável aleatória $\rvx$ com
	\[	a_N(\rvx_1+\rvx_2+\ldots+\rvx_N) - b_N \overset{d}{\rightarrow} \rvx  \]
se e somente se $\rvx$ é $\alpha$-\emph{stable} com $0<\alpha \leq 2$ \cite{taqqu94}. 
\end{teo}
Pode-se mostrar que o teorema (\ref{TLCg}) é conseqüência do teorema (\ref{stable}). O teorema central do limite generalizado  afirma que se a premissa de variância finita for descartada, então o único limite possível é a distribuição estável.  

A definição de distribuição estável pode se estendida para vetores aleatórios \cite{athina04}, \cite{taqqu94}, \cite{shao93}. O vetor $\rvx = [\rvx_1,\rvx_2,\ldots,\rvx_N]$ é um vetor aleatório S$\alpha$S se, e somente se, a combinação linear $a_1\rvx_1 + \ldots + a_N\rvx_N$ é S$\alpha$S para quaisquer $a_1,\ldots,a_N$ reais. Um processo aleatório $\{\rvx_t\}_{t \in \mathbb{Z}}$ é dito $\alpha$-\emph{stable} se para qualquer $N \geq 1$ e índices distintos $t_1,\ldots,t_N$, as variáveis aleatórias $\rvx_{t_1}, \ldots, \rvx_{t_N}$ são conjuntamente $\alpha$-\emph{stable} com o mesmo valor de $\alpha$.
 
\begin{defi}[Processo Estocástico Impulsivo]
\label{def:pe-impulsivo}
Um processo estocástico $\rvx_t$ é impulsivo se a sua distribuição marginal de probabilidades possuir cauda pesada \cite{taqqu94}.
\end{defi}

\subsection{Medidas de Dependência para Processos Impulsivos}

Covariâncias (ou correlações) não podem ser definidas no espaço de variáveis aleatórias estáveis, dado que a variância de uma variável aleatória estável é infinita. Isto não quer dizer que algum tipo de medida de dependência não possa ser definida. De fato, dois tipos de medidas foram propostos na literatura: a \textbf{covariação} e a \textbf{codiferença}. A medida mais utilizada na prática é a codiferença, a qual será definida a seguir. O conceito de covariação não será abordado neste trabalho (a referência \cite{shao93} traz uma exposição detalhada da covariação). 
   
A codiferença de duas variáveis aleatórias conjuntamente S$\alpha$S, $0<\alpha \leq 2$, $\rvx_1$ e $\rvx_2$ é dada por 
\begin{equation}
	\label{eq:codif}
	\gamma_{\rvx_1,\rvx_2} = (\sigma_{\rvx_1})^{\alpha} + (\sigma_{\rvx_2})^{\alpha} - (\sigma_{\rvx_1-\rvx_2})^{\alpha},
\end{equation}
em que $\sigma_{\rvx}$ é o parâmetro de escala da variável  S$\alpha$S $\rvx$. A codiferença é simétrica, ou seja, $\gamma_{\rvx_1,\rvx_2}=\gamma_{\rvx_2,\rvx_1}$ e reduz-se à covariância quando $\alpha=2$. Se $\rvx_1$ e $\rvx_2$ são independentes, então $\gamma_{\rvx_1,\rvx_2}=0$. 

A \textbf{codiferença generalizada} \cite{athina00} é uma extensão da codiferença e é definida para processos com distribuições marginais de cauda pesada genéricas.    

\begin{defi}[Codiferença Generalizada]
\label{def:codif-g}
	\begin{equation}
		I(w_1,w_2;\rvx_1,\rvx_2) = -\ln E[e^{j(w_1 \rvx_1 + w_2 \rvx_2)}] + \ln E[e^{jw_1 \rvx_1}] + \ln E[e^{jw_2 \rvx_2}],\quad (w_1,w_2)\in\mathbb{R}^2
  \end{equation}
\end{defi}

Se $\rvx_1$ e $\rvx_2$ são independentes, então $I(w_1,w_2;\rvx_1,\rvx_2)= 0$. 
Para variáveis aleatórias conjuntamente Gaussianas vale
\[I(w_1,w_2;\rvx_1,\rvx_2)= -w_1w_2 \, C(\rvx_1,\rvx_2),\]
em que $C(\rvx_1,\rvx_2)$ denota a covariância entre $\rvx_1$ e $\rvx_2$. Para o caso dos processos aleatórios estacionários tem-se que  
\[ I(w_1,w_2;\tau)=I(w_1,w_2;\rvx_{t+\tau},\rvx_t).\]

\begin{defi}[Memória Longa em Sentido Generalizado]
\label{def:lrd-g}
Seja $\{\rvx_t\}_{t \in \mathbb{R}}$ um processo estacionário. Diz-se que $\rvx_t$ possui \textbf{memória longa no sentido generalizado}, se a sua codiferença generalizada, $I(w_1,w_2;\tau)|_{w_1=-w_2=1}$ satisfaz 
	\begin{equation}
		\lim_{\tau\rightarrow\infty} I(1,-1;\tau)/\tau^{-\beta} = L(\tau),
  \end{equation}
em que $L(\tau)$ é uma função que varia lentamente para $\tau\rightarrow\infty$ e $0<\beta<1$.  
\end{defi}

Note-se que para processos gaussianos LRD, a definição acima reduz-se à definição clássica de processo LRD
 (vide (\ref{def:lrd2}))\footnote{Os parâmetros $H$, $\alpha$ (não se trata do expoente característico de uma distribuição estável, mas do expoente de escala de um processo auto-similar) e $\beta$ estão relacionados entre si pelas seguintes relações: $\alpha = 2H-1$ e $\beta=2 - 2H$.}.

\section{Por que o tráfego das redes de dados é fractal?}
\label{fractal:sec:fractal?}
A investigação das causas do comportamento fractal do teletráfego não faz parte do escopo desta pesquisa, conforme delineado na seção \ref{cap:intro:sec:obj} desta tese. Não obstante, o autor deste trabalho seria omisso se não fizesse alguns breves comentários sobre essa importante questão, que, como será apontado a seguir, tem sido pesquisada desde o artigo seminal de Leland \emph{et al} \cite{leland94}. 

Vários autores \cite{leland94}, \cite{feldmann98}, \cite{gilbert99}, \cite{crovella96}, \cite{willinger97}, \cite{willinger02} têm afirmado que a auto-similaridade do tráfego Internet seria causada pela grande variabilidade  do tamanho (em \emph{bytes} ou pacotes) das sessões individuais (\ac{FTP}, \ac{HTTP}, etc.) que compõem o tráfego agregado. Mais especificamente, aqueles artigos conjecturam que o tráfego IP é auto-similar porque os tamanhos das sessões individuais que compõem o tráfego Internet são gerados por uma distribuição de probabilidades de cauda pesada. Por outro lado, o recente estudo de Gong \emph{et al} \cite{gong05} reexamina esta questão\footnote{Dentre outros argumentos apresentados, esses autores afirmam que ``a inferência estatística de dados provenientes de uma distribuição de cauda pesada a partir de uma amostra finita de dados é extremamente difícil, senão impossível''.} e advoga que há pouca evidência de que a cauda pesada da distribuição tenha algum impacto sobre o projeto dos algoritmos e a infraestrutura da Internet. Gong \emph{et al} propõem um modelo hierárquico \emph{on-off} Markoviano que explica a LRD do tráfego IP e sustentam que as múltiplas escalas de tempo envolvidas no mecanismo de geração do tráfego\footnote{Por exemplo, um protocolo de transporte como o TCP atua numa escala de tempo de milissegundos, ao passo que eventos associados à camada de aplicação (\emph{browsing}, \emph{downloading}, etc.) acontecem numa escala de tempo da ordem de segundos a minutos.} e os protocolos de transporte (como o \ac{TCP}) fazem com que a observação da LRD seja inevitável. 




%\section{Resumo}
%\label{fractal:sec:resumo}